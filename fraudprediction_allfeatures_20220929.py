# -*- coding: utf-8 -*-
"""DONT_TOUCH FraudPrediction_AllFeatures 20220929.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MoDBLcu8lQ5k04hi5yIz0PGu5hMUBnlH

# Library & Function Loads
"""

#library & db path loads

import pandas as pd
import matplotlib.pyplot as plt

import numpy as np
import datetime as dt
from datetime import timedelta
import seaborn as sns
from sklearn.metrics import silhouette_score


from google.colab import drive
drive.mount('/content/drive')

!pip install pandas-redshift
import pandas_redshift as pr


!pip install connectorx
import connectorx as cx

import urllib.parse
import base64

!pip install pandarallel 
from pandarallel import pandarallel

pandarallel.initialize(progress_bar=True,nb_workers=7)

!pip install shap
import shap

!pip install scikit-optimize
!pip install xgboost==1.6.2 
import xgboost
xgboost.__version__

from skopt.space import Real, Categorical, Integer

pr.connect_to_redshift(
     host='hanadatawarehouse.clv1kgputklq.ap-southeast-1.redshift.amazonaws.com',
     dbname ='hanadb',
     user='ext_rw_user_1',
     password='bA55-f2$8'
  )

finflux_connection = 'mysql://hanareadonly:H%40nare%40d2021@hanareaddb.finflux.io:9423/hana-production'

db_connection_str = 'mysql://kebwebuser:u5erMeb%40pp@hana-production-analytics-read-replica.c0wcwq0ocdj3.ap-southeast-1.rds.amazonaws.com/kebhana_dashboard_db'

xuname="tableausrv@hanamicrofinance.com"
xuname = urllib.parse.quote_plus(xuname)
db_conn_str = 'mysql://' + xuname + ':QrU7w8DxfmbwQpd7@hana-innovation-reader.c4yqwynpry4t.ap-southeast-2.rds.amazonaws.com/hana'

default_date = dt.datetime(1099, 1, 1)
current_date = dt.datetime.now().strftime("%Y-%m-%d %H:%M")

filtered_Positions = ['Branch Manager', 'Assistant Branch Manager','Credit Advisor','Field Officer','Individual Field Officer','Individual Loan Officer']
filtered_Positions2 = ['Field Officer','Individual Field Officer','Individual Loan Officer']

#function declarations

#Load procedures

def PCA_plus_kmeans(df,n_components,n_cluster_override = 0):
  from sklearn.cluster import KMeans
  from sklearn.preprocessing import StandardScaler
  from sklearn.decomposition import PCA

  # Feature Scaling 
  sc = StandardScaler()
  df_scaled = sc.fit_transform(df)


  #PCA
  pca = PCA(n_components = n_components)
  clusteringtable_scaled_pca = pca.fit_transform(df_scaled)
  per_var_captured=np.round(pca.explained_variance_ratio_.sum()*100,1)
  print("Percentage of variance captured:" + str(per_var_captured))

  #find optimum clusters by maximising silhouette score
  silhouette_scores=[]
  for i in range(2,10):
      kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
      kmeans.fit(clusteringtable_scaled_pca)
      silhouette_scores.append(silhouette_score(clusteringtable_scaled_pca, kmeans.labels_, metric='euclidean'))
  plt.bar(x=range(2,10),height=silhouette_scores)

  if n_cluster_override == 0:
    optimumclusters = silhouette_scores.index(max(silhouette_scores))+2 #note: index  = 2 clusters
  else:
     optimumclusters = n_cluster_override
  kmeans = KMeans(n_clusters = optimumclusters, init = 'k-means++', random_state = 42)
  df_with_clusters = df.copy()
  df_with_clusters["Cluster"] = kmeans.fit_predict(clusteringtable_scaled_pca)+1
  df_with_clusters["Cluster"].value_counts()
  print("Silhouette score with PCA (max silhouette): " + str(round(silhouette_score(clusteringtable_scaled_pca, kmeans.labels_, metric='euclidean'),3)))
  print(str(optimumclusters) + " clusters")

  return df_with_clusters, clusteringtable_scaled_pca



def clustering_scatterplot(df_with_clusters,clusteringtable_scaled_pca):
  optimumclusters = len(df_with_clusters["Cluster"].unique())
  if clusteringtable_scaled_pca.shape[1] == 2:
    #2D scatter
    clusteringtable_scaled_pca_df = pd.DataFrame(clusteringtable_scaled_pca,index=df_with_clusters.index,columns=["PC1","PC2"])
    colorarray = ["red","blue","green","magenta","cyan","pink","black"]
    for i in range(1,optimumclusters+1):
        plt.scatter(clusteringtable_scaled_pca_df["PC1"].loc[df_with_clusters["Cluster"]==i],
                                                        clusteringtable_scaled_pca_df["PC2"].loc[df_with_clusters["Cluster"]==i],c=colorarray[i])
    plt.xlabel("PC1")
    plt.ylabel("PC2")

  elif clusteringtable_scaled_pca.shape[1]== 3:
    #3D scatter
    clusteringtable_scaled_pca_df = pd.DataFrame(clusteringtable_scaled_pca,index=df_with_clusters.index,columns=["PC1","PC2","PC3"])
    colorarray = ["red","blue","green","magenta","cyan","pink","black","purple","brown","orange"]
    fig = plt.figure()
    #ax = fig.add_subplot(111, projection='3d')
    ax = plt.axes(projection ="3d")
    ax.view_init(10, 30)
    for i in range(1,optimumclusters+1):
        ax.scatter3D(clusteringtable_scaled_pca_df["PC1"].loc[df_with_clusters["Cluster"]==i],\
                  clusteringtable_scaled_pca_df["PC2"].loc[df_with_clusters["Cluster"]==i],\
                  clusteringtable_scaled_pca_df["PC3"].loc[df_with_clusters["Cluster"]==i],\
                  color = colorarray[i])

    ax.set_xlabel('PC1')
    ax.set_ylabel('PC2')
    ax.set_zlabel('PC3')

    #plt.show()
  
  elif clusteringtable_scaled_pca.shape[1] >3:
    print("More than 3 dimensions detected, please reduce to at most 3 dimensions for visualisation")

def pairplot(df):
  sns.set(style="white")
  sns.pairplot(df)

def crosscorrelation(df):
  f,ax = plt.subplots(figsize=(14, 14))
  sns.heatmap(df.corr(), annot=True, linewidths=.2, fmt= '.1f',ax=ax)

def get_nrc_state(nrc):
  if nrc:
    return int(nrc[:nrc.find("/")])
  return np.nan

def get_ja_effective_date(row, JA_data):
    sorted_JA = sorted(JA_data["effective_date"][(JA_data["employee_id"]==row["employee_id"]) \
                                           & (JA_data["type"]!="resign") \
                                           & (JA_data["location_id"]==JA_data["to_location_id"])])
    if len(sorted_JA) == 0:
        return row["service_join_date"]
    else:
        return np.min(sorted_JA)


def get_tenure_current_location(row):
    xdate = dt.datetime.now()
    if row["status"] != "Active":
        if row["terminated_date"] != default_date:
            xdate = row["terminated_date"]
        else:
            if row["resignation_date"] != default_date:
                xdate = row["resignation_date"]
            #--
        #--
    #--
    eff_date = row["current_location_effective_date"]
    delta = xdate - eff_date
    return delta.days


def get_tenure_current_branch(row):
    xdate = dt.datetime.now()
    eff_date = row["current_branch_effective_date"]
    delta = xdate - eff_date
    return delta.days

def get_staff_last_movement(str_ids):
    df_movements=cx.read_sql(db_conn_str, \
    """
    SELECT 
    e.employee_number, e.name, e.id employee_id,
    cp.name current_position, cl.name current_location, cd.name current_department,
    ja.effective_date, ja.status, ja.type, 
    fr_p.name from_position, to_p.name to_position,
    fr_l.name from_location, to_l.name to_location,
    fr_d.name from_department, to_d.name to_department,
    e.location_id, ja.to_location_id
    FROM hana.job_activities ja
    LEFT JOIN hana.employees e ON e.id = ja.employee_id
    LEFT JOIN hana.statuses s ON s.id = e.status_id
    LEFT JOIN hana.positions cp ON cp.id = e.position_id
    LEFT JOIN hana.positions fr_p ON fr_p.id = ja.from_position_id
    LEFT JOIN hana.positions to_p ON to_p.id = ja.to_position_id
    LEFT JOIN hana.locations cl ON cl.id = e.location_id
    LEFT JOIN hana.locations fr_l ON fr_l.id = ja.from_location_id
    LEFT JOIN hana.locations to_l ON to_l.id = ja.to_location_id
    LEFT JOIN hana.departments cd ON cd.id = e.department_id
    LEFT JOIN hana.departments fr_d ON fr_d.id = ja.from_department_id
    LEFT JOIN hana.departments to_d ON to_d.id = ja.to_department_id
    WHERE ja.employee_id IN ('{ids}')
    ORDER BY e.name, ja.effective_date;
    """.format(ids=str_ids))
    
    df_movements["effective_date"].fillna(default_date, inplace = True)
    return df_movements


def count_past_branchfraud(row,fraudloan_summary):
  if row["branch_code"] in list(fraudloan_summary.index.get_level_values(0)):
    return np.count_nonzero((row["loan_disbursedon_date"]-fraudloan_summary.loc[row["branch_code"]].index).days>0)
  else:
    return 0

def count_past_branchfraud_returncols(row,fraudloan_summary): #returns any number of columns from the fraud summary groupby - can be used to show other fraud details prior to disb date
  if row["branch_code"] in list(fraudloan_summary.index.get_level_values(0)):
    row_at_min_diff = np.count_nonzero((row["loan_disbursedon_date"]-fraudloan_summary.loc[row["branch_code"]].index).days>0)-1
    return fraudloan_summary.loc[row["branch_code"]].iloc[row_at_min_diff][["cumulative_fraudloan_amount_percent_portfolio","cumulative_fraudloan_count_percent_portfolio"]]
  else:
    return pd.Series([0,0])

def get_scorecard_stats (xgbmodel,x_test, y_test,output):

  y_pred3 = xgbmodel.predict_proba(x_test)[:,1]

  #HISTOGRAM


  import matplotlib.pyplot as plt

  # An "interface" to matplotlib.axes.Axes.hist() method
  n, bins, patches = plt.hist(x=y_pred3, bins='auto', color='#0504aa',
                              alpha=0.7, rwidth=0.85)
  plt.grid(axis='y', alpha=0.75)
  plt.xlabel('Pd')
  plt.ylabel('Frequency')
  plt.title('Test dataset Pd distribution')
  plt.text(23, 45, r'$\mu=15, b=3$')
  maxfreq = n.max()
  # Set a clean upper y-axis limit.
  plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)
  print("base score (distribution avg) = " + str(output.mean()))

def getsavingbalance_atdisbdate(row):
  if row['loan_client_id'] in m_savings_balance.index:
    return m_savings_balance.loc[[row['loan_client_id']]][
        m_savings_balance.loc[[row['loan_client_id']]]["transaction_date"]<=row['loan_disbursedon_date']].groupby(["saving_id"])["running_balance_derived"].last().sum()
  else:
    return 0


def getloanbalance_atdisbdate(row):
  if row['loan_client_id'] in m_loan_balance.index:
    return m_loan_balance.loc[[row['loan_client_id']]][
        m_loan_balance.loc[[row['loan_client_id']]]["transaction_date"]<=(row['loan_disbursedon_date']+timedelta(days=3))].groupby(["loan_id"])["outstanding_loan_balance_derived"].last().sum()
  else:
    return 0

def topn_mostcorr_features(n,df):
  test = df.corr().abs().unstack()
  test = test.sort_values(ascending=False).reset_index()
  test = test[test["level_0"]!=test["level_1"]]
  test = test.iloc[::2]
  return test.head(n)

def featureinvestigation(featurename):
  agglevelmap = {
    "branchagg": "branch_code",
    "FOIDagg": "loan_staff_id",
    "clientidagg": "loan_client_id",
    "loanidagg" : "loan_id"
  }

  agglevelmapfr = {
    "branchagg": "branch_code",
    "FOIDagg": "loan_staff_id",
    "clientidagg": "finflux_client_id",
    "loanidagg" : "loan_id"
  }

  aggfield = agglevelmap.get(featurename.split('_')[0])
  aggfieldfr = agglevelmapfr.get(featurename.split('_')[0])

  temp = scoring_df.groupby(aggfield)[featurename].last().to_frame()

  plt.figure(1)
  plt.hist(temp[featurename])
  plt.ylabel("frequency")
  plt.xlabel(featurename)

  fraudloanid_df_1 = fraudloanid_df.merge(m_loan[["loan_staff_id","loan_id"]],how="left",on="loan_id")

  fraudloanid_df_1 = fraudloanid_df_1[fraudloanid_df_1["Status"]!="Clear"].groupby(aggfieldfr).agg({
      "CaseID":"nunique","ClientID":"count","loan_principal_amount":"sum"})
  fraudloanid_df_1.columns = ["n_cases","loan_count","total_fraud_principal"]

  temp = temp.merge(fraudloanid_df_1,how="left",left_index=True,right_index=True)
  temp = temp.fillna(0)
  temp["had_fraud"] = (temp["n_cases"]>0)

  plt.figure(2)
  temp1 = temp.groupby("had_fraud")[featurename].mean()
  plt.bar(list(temp1.index.astype(str)), list(temp1.values))
  plt.ylabel(featurename)
  plt.xlabel("had_fraud")

  temp["Pf"] = scoring_df.groupby(aggfield)["Pf1"].mean()
  plt.figure(3)
  plt.scatter(y=temp["Pf"] , x = temp[featurename])
  plt.ylabel("Prob. of fraud - Pf")
  plt.xlabel(featurename)

"""# DB Table Loads"""

#all DB table loads

print("loading small datasets")

masterusermapping = pr.redshift_to_pandas("select * from extrinsic_db.t_user_key ")
masterusermapping.columns = ['id','employee_number',
 'Name',
 'Email',
 'NRC',
 'MSISDN',
 'FinfluxID',
 'FOID',
 'BranchName',
 'BranchCode',
 'created_by',
 'created_at',
 'updated_by',
 'updated_at',
 'client_id',
 'status']

masterusermapping.drop_duplicates(subset="employee_number", keep="first", inplace=True)
masterusermapping["employee_number"] = masterusermapping["employee_number"].astype(float)
masterusermapping["MSISDN"] = masterusermapping["MSISDN"].fillna(np.nan).replace("",np.nan).astype(float)
staffloan_clientidlist = masterusermapping[masterusermapping["client_id"].notnull()]["client_id"]
masterusermapping_clientidnotnull = masterusermapping[masterusermapping["client_id"].notnull()][["client_id","FOID"]]
masterusermapping_foidnotnull = masterusermapping[masterusermapping["FOID"].notnull()]
masterusermapping_foidnotnull.drop_duplicates(subset="FOID", keep="first", inplace=True)
masterusermapping_foidnotnull.to_csv("FOID mapping.csv")


towndistrictmapping = pr.redshift_to_pandas("select * from extrinsic_db.t_town_location")

statecodes = list(range(1,16))
statelist=["Kachin","Kayah","Kayin","Chin","Sagaing","Tanintharyi","Bago","Magway","Mandalay","Mon","Rakhine","Yangon","Shan","Ayeyawady","Nay Pyi Taw"]
MMstatecode_df = pd.DataFrame(list(zip(statecodes,statelist)),columns = ["StateCode","StateName"])

masterbranchmapping = pr.redshift_to_pandas("select * from extrinsic_db.t_branch_location")
masterbranchmapping = masterbranchmapping.merge(towndistrictmapping[["township_name_eng","sr_name_eng"]].drop_duplicates(subset="township_name_eng"),how="left",left_on="township_name",right_on="township_name_eng")
masterbranchmapping.loc[masterbranchmapping["township_name"]=="Aungpan","sr_name_eng"] = "Shan"
masterbranchmapping.loc[masterbranchmapping["township_name"]=="Shwenyaung","sr_name_eng"] = "Shan"
masterbranchmapping.loc[masterbranchmapping["sr_name_eng"].str.contains("Shan"),"sr_name_eng"] = "Shan"
masterbranchmapping.loc[masterbranchmapping["sr_name_eng"].str.contains("Bago"),"sr_name_eng"] = "Bago"
masterbranchmapping.loc[masterbranchmapping["sr_name_eng"].str.contains("Ayeyarwady"),"sr_name_eng"] = "Ayeyawady"
masterbranchmapping = masterbranchmapping.merge(MMstatecode_df,how="left",left_on = "sr_name_eng", right_on = "StateName")


emailactivity = pr.redshift_to_pandas("select * from extrinsic_db.t_email_activity")
emailactivity = emailactivity.loc[emailactivity["branch_code"].str.len()==4]

teamsactivity = pr.redshift_to_pandas("select * from extrinsic_db.t_msteam_activity")
teamsactivity = teamsactivity.loc[teamsactivity["branch_code"].str.len()==4]


mailboxusage = pr.redshift_to_pandas("select * from extrinsic_db.t_mailbox_usage")
mailboxusage = mailboxusage.loc[mailboxusage["branch_code"].str.len()==4]

sharepointactivity = pr.redshift_to_pandas("select * from extrinsic_db.t_sharepoint_activity")
sharepointactivity = sharepointactivity.loc[sharepointactivity["branch_code"].str.len()==4]
sharepointactivity["SyncSharedFileCount"] = sharepointactivity[["synced_file_count","shared_internal_file_count",
                                                                    "shared_external_file_count"]].sum(axis=1)

loginactivity = pr.redshift_to_pandas("select * from extrinsic_db.t_login_log")
loginactivity["creation_date"] = loginactivity["creation_date"].dt.date

auditactivity = pr.redshift_to_pandas("select * from extrinsic_db.t_audit")

phoneactivity = pr.redshift_to_pandas("select * from extrinsic_db.t_mobile_usage")
phoneactivity["msisdn"] = phoneactivity["msisdn"].astype(float)
phoneactivity["data_used_mb"] = phoneactivity["data_used_mb"].astype(float)
phoneactivity=pd.merge(masterusermapping[["MSISDN","employee_number","FOID"]],phoneactivity,
                                    left_on="MSISDN",right_on="msisdn",how="right")
phoneactivity = phoneactivity[pd.notnull(phoneactivity["employee_number"])]
phoneactivity["employee_number"] = phoneactivity["employee_number"].astype(float)


conflictactivity = pr.redshift_to_pandas(" select * from extrinsic_db.t_acled_myanmar WHERE year = 2022 AND event_type != 'Riots' ")

census_2019_data = pr.redshift_to_pandas("select * from extrinsic_db.t_census_summary ")

taxdata=cx.read_sql(db_conn_str, """SELECT e.employee_number, e.parents_count number_of_parent, e.child_count number_of_child, e.has_spouse \
FROM employees e \
WHERE e.employee_number IS NOT NULL \
ORDER BY e.employee_number; """)
taxdata['employee_number']=taxdata['employee_number'].str.decode(encoding = 'UTF-8').astype(float)

# taxdata = pr.redshift_to_pandas("select employee_number, number_of_parent,number_of_child, has_spouse from extrinsic_db.t_employee_tax ")
# taxdata["employee_number"] = taxdata["employee_number"].astype(float)

disciplinarydata = pr.redshift_to_pandas("select * from extrinsic_db.t_employee_disciplinary")
disciplinarydata["warning_type"] = disciplinarydata["warning_type"].str.lower().replace("first warning","first letter warning").replace("letter warning","first letter warning").replace("dismiss letter","termination letter")
disciplinarydata["employee_number"] = disciplinarydata["employee_number"].fillna(0)
disciplinarydata["employee_number"] = disciplinarydata["employee_number"].replace("",np.nan)
disciplinarydata["employee_number"] = disciplinarydata["employee_number"].astype(float)

employeesurveydata = pr.redshift_to_pandas("select * from extrinsic_db.t_employee_survey")[["employee_number","category"]]
employeesurveydata["category"] = employeesurveydata["category"].replace('3000','ShortTerm').replace('50000','LongTerm').replace('None','NoAnswer')
employeesurveydata.columns=["employee_number","PsychoTestAnswer"]

BM_externalhire = pr.redshift_to_pandas("select * from extrinsic_db.t_employment_status")[["employee_number","branch_name","hire_from"]]
BM_externalhire.columns = ["employee_number","branch_name","BM_externalhire"]
BM_externalhire["BM_externalhire"] = BM_externalhire["BM_externalhire"].replace("Old","False")
BM_externalhire["BM_externalhire"] = BM_externalhire["BM_externalhire"].replace("New","True")
BM_externalhire["employee_number"] = BM_externalhire["employee_number"].astype(float)

BM_MFIexperience = pr.redshift_to_pandas("select * from extrinsic_db.t_mfi_experience")
BM_MFIexperience["employee_number"] = BM_MFIexperience["employee_number"].astype(float)

client_address = cx.read_sql(finflux_connection,\
                             "SELECT c.id AS address_client_id,av.name AS address_ward_village,ae.address_id,ae.last_updated_date AS address_last_updated_date \
FROM m_client c \
LEFT JOIN f_address_entity ae ON ae.entity_id=c.id \
LEFT JOIN f_address a ON a.id = ae.address_id \
LEFT JOIN f_address_region_value av ON av.id = a.address_region_value_id;")
client_address.drop_duplicates(subset="address_client_id", keep="last", inplace=True)



m_client = cx.read_sql(finflux_connection,\
                       "SELECT c.id AS client_id,c.mobile_no AS client_mobile_no,c.alternate_mobile_no AS client_alternate_mobile_no,c.staff_id AS client_staff_id,c.office_id AS client_office_id,c.status_enum AS client_status_enum,c.activation_date AS client_activation_date,c.submittedon_date AS client_submittedon_date \
FROM m_client c;")

m_client = pd.merge(m_client, client_address[['address_client_id','address_ward_village','address_last_updated_date']] ,how='left', left_on=['client_id'], right_on=['address_client_id'])
m_client['client_today_date'] = pd.to_datetime("today")
m_client['address_last_updated_date'] = pd.to_datetime(m_client['address_last_updated_date']).dt.date
m_client['client_submittedon_date'] = pd.to_datetime(m_client['client_submittedon_date']).dt.date
m_client['client_tenure'] = (m_client['client_today_date'] - m_client['client_activation_date']) / np.timedelta64(1, 'Y')
m_client['client_urban'] = m_client['address_ward_village'].str.contains('Ward', na=False)
m_client['client_address_change'] = np.where(((m_client['address_last_updated_date'] - m_client['client_submittedon_date'])/ np.timedelta64(1, 'D'))>30,1,0)
m_client['client_ph_no'] = np.where(m_client['client_alternate_mobile_no'].notnull(),m_client['client_alternate_mobile_no'],m_client['client_mobile_no'])
m_client['client_ph_duplicated'] = np.where((m_client['client_ph_no'].duplicated(keep=False))&(m_client['client_ph_no'].notnull())&(m_client['client_ph_no']!=None)&(m_client['client_ph_no']!='#N/A'),1,0)

m_staff =  cx.read_sql(finflux_connection,\
                       "SELECT s.id AS staff_id,s.office_id AS staff_office_id,s.joining_date AS staff_joining_date \
FROM m_staff s;")

print("loading m_loan_transaction")

loan_transaction_agg_office = cx.read_sql(finflux_connection,
                                          "SELECT lt.office_id AS loan_transaction_office_id,COUNT(lt.id) AS loan_transaction_count_office,SUM(lt.is_reversed) AS loan_transaction_reversed_count_office \
                                          FROM m_loan_transaction lt \
                                          GROUP BY lt.office_id;")

print("loading m_loan")

# m_loan = cx.read_sql(finflux_connection,\
#             "SELECT l.id AS loan_id,l.loan_status_id,l.client_id AS loan_client_id,l.loan_officer_id AS loan_staff_id,l.approvedon_date AS loan_approvedon_date,l.disbursedon_date AS loan_disbursedon_date,l.principal_amount AS loan_principal_amount, l.principal_outstanding_derived \
# FROM m_loan l WHERE l.loan_status_id NOT IN (100,200,500);")
m_loan = cx.read_sql(finflux_connection,"SELECT l.id AS loan_id,l.loan_status_id,l.client_id AS loan_client_id,l.loan_officer_id AS loan_staff_id, \
l.approvedon_date AS loan_approvedon_date,l.disbursedon_date AS loan_disbursedon_date, \
l.principal_amount AS loan_principal_amount, l.principal_outstanding_derived,la.loan_officer_id AS loan_staff_id_atdisb \
FROM m_loan l \
LEFT JOIN (SELECT loa.loan_id,loa.loan_officer_id \
FROM m_loan_officer_assignment_history AS loa \
WHERE loa.id IN (SELECT MIN(id) FROM m_loan_officer_assignment_history GROUP BY loan_id) \
) AS la ON la.loan_id=l.id \
WHERE l.loan_status_id NOT IN (100,200,500);")
# m_loan["loan_staff_id"] = m_loan["loan_staff_id"].fillna(0)
m_loan["loan_staff_id_atdisb"] = m_loan["loan_staff_id_atdisb"].fillna(0)
m_loan["loan_staff_id"] = m_loan["loan_staff_id_atdisb"]
m_loan["loan_client_id"] = m_loan["loan_client_id"].fillna(0)
m_loan = m_loan.merge(m_client[["client_office_id","client_id"]],how="left",left_on = "loan_client_id", right_on = "client_id")
m_loan = m_loan.merge(masterbranchmapping[["branch_code","office_id"]],how="left",left_on="client_office_id",right_on = "office_id")
m_loan = pd.merge(m_loan, m_staff[['staff_id','staff_joining_date']] ,how='left', left_on=['loan_staff_id'], right_on=['staff_id'])
m_loan['loan_disbursed_year_month'] = pd.to_datetime(m_loan['loan_disbursedon_date']).dt.to_period('M')
m_loan['loan_staff_firstyear'] = np.where((((m_loan['loan_disbursedon_date'] - m_loan['staff_joining_date']) / np.timedelta64(1, 'Y'))<=1)&(((m_loan['loan_disbursedon_date'] - m_loan['staff_joining_date']) / np.timedelta64(1, 'Y'))>0),1,0)

m_loan_officer_assignment_history = cx.read_sql(finflux_connection,"SELECT ah.loan_id AS assignment_loan_id,ah.loan_officer_id AS assignment_staff_id,ah.start_date AS assignment_start_date,ah.end_date AS assignment_end_date \
FROM m_loan_officer_assignment_history ah;")
m_loan_officer_assignment_history = pd.merge(m_loan_officer_assignment_history, m_loan[['loan_id','loan_approvedon_date','loan_client_id']] ,how='left', left_on=['assignment_loan_id'], right_on=['loan_id'])
m_loan_officer_assignment_history['assignment_reassigned_case'] = np.where(((m_loan_officer_assignment_history['loan_approvedon_date'] - m_loan_officer_assignment_history['assignment_start_date'])/ np.timedelta64(1, 'D'))!=0,1,0)
m_loan_officer_assignment_history['assignment_case_reassigned'] = np.where(m_loan_officer_assignment_history['assignment_end_date'].notnull(),1,0)

m_office = cx.read_sql(finflux_connection,\
                         "SELECT o.id AS office_id,o.external_id As office_code,o.name AS office_name, o.opening_date as opening_date \
FROM m_office o;")

acc_gl_closure = cx.read_sql(finflux_connection,\
                             "SELECT * FROM acc_gl_closure cl;")
acc_gl_closure["closing_delay_days"] = round((acc_gl_closure["created_date"].dt.date - acc_gl_closure["closing_date"].dt.date).dt.days,0)
acc_gl_closure["last_closing_date_delay_days"] = round((dt.datetime.today().date() - acc_gl_closure["closing_date"].dt.date).dt.days,0)
acc_gl_closure["reopen_delay_days"] = round((acc_gl_closure["last_updated_date"].dt.date - acc_gl_closure["created_date"].dt.date).dt.days,0)


intervention_dataset = pd.read_csv('/content/drive/My Drive/Processing Inputs/intervention_dataset_15Sep.csv')
intervention_dataset.drop_duplicates(subset="latedays_loan_id", keep="last", inplace=True)

print("loading digital payments")
digital_payment_data = pr.redshift_to_pandas("select * from extrinsic_db.t_digital_payment")
digital_payment_data["branch_code"] = digital_payment_data["branch_code"].str.upper()
digital_payment_data["branch_code"] = digital_payment_data["branch_code"].replace("YAY","YAYY")
digital_payment_data=digital_payment_data[pd.to_numeric(digital_payment_data['loan_id'], errors='coerce').notnull()]
digital_payment_data["loan_id"] = digital_payment_data["loan_id"].astype(float)
digital_payment_data["amount"] = digital_payment_data["amount"].astype(float)
digital_payment_data = digital_payment_data.merge(m_loan[["loan_id","loan_staff_id","loan_principal_amount","loan_disbursedon_date"]],how="left",
                                                  on="loan_id")


print("loading ddfr")
hwa_ddfr = cx.read_sql(db_connection_str,"SELECT d.uuid AS ddfr_id,d.client_id AS ddfr_client_id,dl.m_loan_id AS ddfr_loan_id,c.staff_id AS ddfr_staff_id,d.root_cause AS ddfr_root_cause,d.follow_up_result AS ddfr_follow_up_result,d.created_at as ddfr_created_at \
FROM kebhana_middleware_db.hwa_ddfr AS d \
LEFT JOIN kebhana_middleware_db.hwa_ddfr_loans AS dl ON dl.hwa_ddfr_uuid=d.uuid \
LEFT JOIN kebhana_middleware_db.m_client AS c ON c.id=d.client_id \
where d.created_at >= '2022-08-05';")

print("loading late days")

hwa_late_day_fields = cx.read_sql(db_connection_str,"SELECT l.loan_id AS latedays_loan_id,l.client_id AS latedays_client_id,gc.group_id AS latedays_group_id,o.id AS latedays_office_id,o.external_id AS latedays_office_code,l.late_days_total,l.paid_pc_paid AS latedays_paid_pc_paid,l.paid_pc_due_interestprincipal AS latedays_paid_pc_due_interestprincipal,l.reschedule_count_total AS latedays_reschedule_count \
FROM kebhana_middleware_db.hwa_late_day_fields AS l \
LEFT JOIN kebhana_middleware_db.m_client AS c ON c.id=l.client_id \
LEFT JOIN kebhana_middleware_db.m_group_client AS gc ON gc.client_id=c.id \
LEFT JOIN kebhana_middleware_db.m_office AS o ON o.id=c.office_id;")
hwa_late_day_fields.drop_duplicates(subset="latedays_loan_id", keep="last", inplace=True)
hwa_late_day_fields['latedays_paid_pc_paid'] = hwa_late_day_fields['latedays_paid_pc_paid'].astype('int64')
hwa_late_day_fields['latedays_paid_pc_due_interestprincipal'] = hwa_late_day_fields['latedays_paid_pc_due_interestprincipal'].astype('int64')
hwa_late_day_fields['late_days_total'] = hwa_late_day_fields['late_days_total'].astype('int64')
hwa_late_day_fields['latedays_reschedule_count'] = hwa_late_day_fields['latedays_reschedule_count'].astype('int64')

#active portfolio groupings

activeportfolio_by_branch = m_loan[m_loan["loan_status_id"]==300].groupby("branch_code").agg({
    "loan_principal_amount": "sum","loan_status_id": "count"})
activeportfolio_by_branch.columns = ["branch_portfolio_amount","branch_loan_count"]

activeportfolio_by_FO = m_loan[m_loan["loan_status_id"]==300].groupby("loan_staff_id").agg({
    "loan_principal_amount": "sum","loan_status_id": "count"})
activeportfolio_by_FO.columns = ["FO_portfolio_amount","FO_caseload"]

print("loading partial repayment dataset")
today=pd.to_datetime("today").strftime('%Y-%m-%d')
loan_transaction = cx.read_sql(finflux_connection,"SELECT r.loan_id,COUNT(distinct(rt.loan_transaction_id)) AS transaction_count \
FROM m_loan_repayment_schedule r \
LEFT JOIN m_loan_transaction_repayment_schedule_mapping rt ON rt.loan_repayment_schedule_id=r.id \
LEFT JOIN m_loan_transaction t ON t.id=rt.loan_transaction_id \
WHERE t.is_reversed<>1 \
GROUP BY r.loan_id;")
loan_repayment = cx.read_sql(finflux_connection,"SELECT r.loan_id,COUNT(r.id) AS due_repayment_count \
FROM m_loan_repayment_schedule r \
WHERE r.duedate<='"+today+"' \
AND (r.principal_amount<>0 OR r.interest_amount<>0) \
GROUP BY r.loan_id;")


print("loading BetterHR")
#BetterHR
str_positions = "','".join([str(id) for id in filtered_Positions])

df_FO=cx.read_sql(db_conn_str, \
"""
SELECT 
e.employee_number, e.gender, e.date_of_birth, e.nrc,  
e.service_join_date, e.terminated_date, e.resignation_date, 
(YEAR('{curr_date}') - YEAR(e.date_of_birth)) empe_age,
IF(e.status_id IN ( '85f97410-0b5a-11ea-b807-c38ec09e98e8'), DATEDIFF( '{curr_date}' , e.service_join_date),
 IF (e.resignation_date IS NOT NULL, DATEDIFF( e.resignation_date , e.service_join_date), 
    IF (e.terminated_date IS NOT NULL, DATEDIFF( e.terminated_date , e.service_join_date), NULL) )
) tenure_hana_days,
e.id employee_id, st.name status
FROM employees e 
LEFT JOIN departments d ON d.id = e.department_id
LEFT JOIN positions p ON p.id = e.position_id
LEFT JOIN locations l on l.id = e.location_id
LEFT JOIN statuses st ON st.id = e.status_id
WHERE 1=1
AND p.name IN ('{pos}')
AND e.employee_number IS NOT NULL
ORDER BY l.name, e.name;
""".format(curr_date=current_date, pos=str_positions))

df_FO['employee_number']=df_FO['employee_number'].str.decode(encoding = 'UTF-8').astype(float)
df_FO["terminated_date"].fillna(default_date, inplace = True)
df_FO["resignation_date"].fillna(default_date, inplace = True)
df_FO["nrc_state"] = df_FO["nrc"].apply(get_nrc_state)

empeid_list = df_FO["employee_id"].tolist()


str_empe_ids = "','".join([str(id) for id in empeid_list])

df_JA=cx.read_sql(db_conn_str, \
"""
SELECT 
e.employee_number, e.name, e.id employee_id,
cp.name current_position, cl.name current_location, cd.name current_department,
ja.effective_date, ja.status, ja.type, ja.transfer_format, ja.employee_request, ja.career_progression,
fr_p.name from_position, to_p.name to_position,
fr_l.name from_location, to_l.name to_location,
fr_d.name from_department, to_d.name to_department,
e.location_id, ja.to_location_id
FROM hana.job_activities ja
LEFT JOIN hana.employees e ON e.id = ja.employee_id
LEFT JOIN hana.statuses s ON s.id = e.status_id
LEFT JOIN hana.positions cp ON cp.id = e.position_id
LEFT JOIN hana.positions fr_p ON fr_p.id = ja.from_position_id
LEFT JOIN hana.positions to_p ON to_p.id = ja.to_position_id
LEFT JOIN hana.locations cl ON cl.id = e.location_id
LEFT JOIN hana.locations fr_l ON fr_l.id = ja.from_location_id
LEFT JOIN hana.locations to_l ON to_l.id = ja.to_location_id
LEFT JOIN hana.departments cd ON cd.id = e.department_id
LEFT JOIN hana.departments fr_d ON fr_d.id = ja.from_department_id
LEFT JOIN hana.departments to_d ON to_d.id = ja.to_department_id
WHERE ja.employee_id IN ('{ids}')
ORDER BY e.name, ja.effective_date;
""".format(ids=str_empe_ids))

df_JA['employee_number']=df_JA['employee_number'].str.decode(encoding = 'UTF-8')
df_JA["effective_date"].fillna(default_date, inplace = True)

df_FO["current_location_effective_date"] = df_FO.apply(get_ja_effective_date, JA_data=df_JA, axis = 1)
df_FO["tenure_current_location_days"] = df_FO.apply(get_tenure_current_location, axis = 1)


df_MS=cx.read_sql(db_conn_str, \
"""
SELECT ecf.employee_id, ecf.data martial_status
FROM employee_custom_field_v2 AS ecf
WHERE ecf.custom_field_id = 'ab546553-21ed-4b60-8ea3-62ee651d42af' 
AND ecf.employee_id IN ('{ids}')
""".format(ids=str_empe_ids))

df_MS['martial_status']=df_MS['martial_status'].str.decode(encoding = 'UTF-8')
df_FO = pd.merge(df_FO, df_MS,on=["employee_id"], how="left")


str_positions2 = "','".join([str(id) for id in filtered_Positions2])

df_FO2=cx.read_sql(db_conn_str, \
"""
SELECT 
    V.branch_name, V.location_id, 
    AVG(V.empe_age) fo_avg_age,
    AVG(V.tenure_hana_days) fo_avg_tenure_hana_days,
    SUM( IF(V.gender='female', 1, 0) ) + SUM( IF(V.gender='male', 1, 0) ) total_fo_count,
    SUM( IF(V.gender='female', 1, 0) ) / SUM( IF(V.gender='male', 1, 0) ) fo_female_to_male_gender_ratio 
FROM (
    SELECT 
    l.name branch_name, l.id location_id, e.employee_number, e.nrc,  e.name, e.gender, e.date_of_birth, 
    st.name status, p.name position_name,
    (YEAR('{curr_date}') - YEAR(e.date_of_birth)) empe_age,
    DATEDIFF( '{curr_date}' , e.service_join_date) tenure_hana_days 
    FROM employees e 
    LEFT JOIN positions p ON p.id = e.position_id
    LEFT JOIN locations l on l.id = e.location_id
    LEFT JOIN statuses st ON st.id = e.status_id
    WHERE 1=1
    AND p.name IN ('{pos}')
    AND e.status_id IN ( '85f97410-0b5a-11ea-b807-c38ec09e98e8')
) V
GROUP BY V.branch_name
ORDER BY V.branch_name;
""".format(curr_date=current_date, pos=str_positions2))

branch_list = df_FO2["location_id"].tolist()
str_branch_ids = "','".join([str(id) for id in branch_list])

df_BM=cx.read_sql(db_conn_str, \
"""
    SELECT 
    l.name branch_name, e.location_id, e.gender bm_gender, e.id employee_id, 
    e.service_join_date,
    (YEAR('{curr_date}') - YEAR(e.date_of_birth)) bm_age,
    DATEDIFF( '{curr_date}' , e.service_join_date) bm_tenure_hana_days 
    FROM employees e 
    LEFT JOIN positions p ON p.id = e.position_id
    LEFT JOIN locations l on l.id = e.location_id
    LEFT JOIN statuses st ON st.id = e.status_id
    WHERE 1=1
    AND p.name IN ('Branch Manager')
    AND e.status_id IN ( '85f97410-0b5a-11ea-b807-c38ec09e98e8')
    ORDER BY l.name;
""".format(curr_date=current_date))

df_ABM=cx.read_sql(db_conn_str, \
"""
SELECT 
    e.location_id, e.gender abm_gender, e.id employee_id,
    e.service_join_date,
    (YEAR('{curr_date}') - YEAR(e.date_of_birth)) abm_age,
    DATEDIFF( '{curr_date}' , e.service_join_date) abm_tenure_hana_days
FROM employees e 
WHERE 1=1
    AND e.position_id IN ('993e29a0-0bb0-11ea-bd60-afa5d95311cb') -- ABM
    AND e.status_id IN ( '85f97410-0b5a-11ea-b807-c38ec09e98e8') -- Active
    AND e.location_id IN ('{ids}')
ORDER BY e.location_id;
""".format(ids=str_branch_ids, curr_date=current_date))


df_CA=cx.read_sql(db_conn_str, \
"""
SELECT 
     e.location_id, e.id employee_id, e.gender, 
     e.service_join_date,
    (YEAR(current_date()) - YEAR(e.date_of_birth)) empe_age,
    DATEDIFF( current_date() , e.service_join_date) tenure_hana_days
FROM employees e 
WHERE 1=1
    AND e.position_id IN ('982a4660-0bb0-11ea-b21b-c314fced9a68') -- CA
    AND e.status_id IN ( '85f97410-0b5a-11ea-b807-c38ec09e98e8') -- Active
    AND e.location_id IN ('{ids}')
ORDER BY e.location_id;
""".format(ids=str_branch_ids, curr_date=current_date))

print("loading fruad loan dataset")
#fraud loan db
fraudloanid_df = pd.read_csv('/content/drive/My Drive/Processing Inputs/fraud_loanid_allbranches_20220920.csv')
fraudloanid_df["disbursedon_date"] = pd.to_datetime(fraudloanid_df["disbursedon_date"])
fraudloanid_df["Fraud report date"] = pd.to_datetime(fraudloanid_df["Fraud report date"])
fraudloanid_df["Branch"] = fraudloanid_df["Branch"].str.strip() 

fraudloanid_df = fraudloanid_df.merge(masterbranchmapping[["description","branch_code"]],how="left", left_on="Branch",right_on="description")
fraudloanid_df = fraudloanid_df.merge(m_loan[["loan_id","loan_principal_amount"]],how="left", left_on="finflux_loan_id",right_on="loan_id")
fraudloanidlist = fraudloanid_df.loc[fraudloanid_df["Status"]!="Clear","finflux_loan_id"]
print("no. of fraud loans found: " + str(len(fraudloanidlist)))

#print("loading m_savings_balance")

# m_savings_balance = cx.read_sql(finflux_connection,"SELECT sa.client_id, sa.id AS saving_id, transaction_date, transaction_type_enum , amount, running_balance_derived \
# FROM m_savings_account_transaction sat , m_savings_account sa \
# WHERE sa.id = sat.savings_account_id \
# AND transaction_type_enum IN (1,2,3) \
# AND sat.is_reversed = 0 \
# AND transaction_date >= '2019-01-01'\
# ORDER BY client_id, saving_id, transaction_date")
# m_savings_balance = m_savings_balance[m_savings_balance["client_id"].notnull()]
# m_savings_balance["running_balance_derived"] = m_savings_balance["running_balance_derived"].fillna(0)
# m_savings_balance = m_savings_balance.set_index("client_id")

# m_savings_balance = pd.read_csv("/content/drive/My Drive/Processing Inputs/m_savings_balance.csv")
# m_savings_balance["transaction_date"] = pd.to_datetime(m_savings_balance["transaction_date"])
# m_savings_balance = m_savings_balance.set_index("client_id")
# # print("loading m_loan_balance")

# m_loan_balance = cx.read_sql(finflux_connection,"SELECT l.client_id, l.id loan_id, lt.transaction_date, lt.outstanding_loan_balance_derived \
# FROM m_loan l \
# INNER JOIN m_loan_transaction lt on lt.loan_id = l.id \
# INNER JOIN r_enum_value en on en.enum_name='loan_transaction_type_enum' AND en.enum_id= lt.transaction_type_enum \
# AND lt.transaction_type_enum IN (1,2) \
# AND lt.is_reversed = 0 \
# AND lt.transaction_date >= '2020-01-01' \
# ORDER BY l.client_id, l.id, lt.transaction_date")
# m_loan_balance = m_loan_balance[m_loan_balance["client_id"].notnull()]
# m_loan_balance["outstanding_loan_balance_derived"] = m_loan_balance["outstanding_loan_balance_derived"].fillna(0)
# m_loan_balance = m_loan_balance.set_index("client_id")

# m_loan_balance = pd.read_csv("/content/drive/My Drive/Processing Inputs/m_loan_balance.csv")
# m_loan_balance["transaction_date"] = pd.to_datetime(m_loan_balance["transaction_date"])
# m_loan_balance = m_loan_balance.set_index("client_id")

#saving_loanagg = m_loan[m_loan["loan_disbursedon_date"]>=dt.datetime(2020,1,1)][["loan_id","loan_disbursedon_date","loan_client_id","loan_staff_id","branch_code","loan_principal_amount"]]
#saving_loanagg["savingsbalance_atdisbursement"] = saving_loanagg.parallel_apply(getsavingbalance_atdisbdate,axis=1)
#saving_loanagg["loanbalance_atdisbursement"] = saving_loanagg.parallel_apply(getloanbalance_atdisbdate,axis=1)
# saving_loanagg["loanbalance_atdisbursement_withcurrentdisb"] = saving_loanagg["loanbalance_atdisbursement"]
# saving_loanagg.loc[saving_loanagg["loanbalance_atdisbursement_withcurrentdisb"]==0,"loanbalance_atdisbursement_withcurrentdisb"] = saving_loanagg.loc[saving_loanagg["loanbalance_atdisbursement_withcurrentdisb"]==0,"loan_principal_amount"]
#saving_loanagg["savings_loans_percent_atdisbursement"] = round(100*saving_loanagg["savingsbalance_atdisbursement"] / saving_loanagg["loanbalance_atdisbursement_withcurrentdisb"],1)
#saving_loanagg.to_csv("/content/drive/My Drive/Processing Inputs/savingsbalance_at_loandisb.csv")

m_saving_current = cx.read_sql(finflux_connection,"SELECT id, client_id, account_balance_derived FROM m_savings_account ORDER BY client_id")

saving_loanagg = pd.read_csv("/content/drive/My Drive/Processing Inputs/savingsbalance_at_loandisb.csv")
saving_loanagg["loan_disbursedon_date"] = pd.to_datetime(saving_loanagg["loan_disbursedon_date"])
saving_loanagg = saving_loanagg.drop(columns = "Unnamed: 0")

"""# Aggregations"""

#BRANCH LEVEL AGGREGATIONS

ALL_BRANCHCODE_AGG=pd.DataFrame(index = masterbranchmapping["branch_code"])

#BranchTechUsageCluster
loginactivity_internal = loginactivity.loc[(loginactivity["user_type"]=="Internal")&
                                           (~loginactivity["user_id"].str.contains("#EXT#"))&
                                           (~loginactivity["user_id"].str.contains("-")),:]

loginactivity_internal = loginactivity_internal.sort_values(by=["user_id","creation_date"]).reset_index()
useridlist = loginactivity_internal["user_id"].unique()
loginactivity_reportdate = loginactivity_internal["creation_date"].max()

for name in useridlist:
    loginactivity_internal.loc[loginactivity_internal["user_id"]==name,"ShiftCreationDateConv"] = loginactivity_internal.loc[loginactivity_internal["user_id"]==name,"creation_date"].shift(1)
    loginactivity_internal["ShiftCreationDateConv"] = pd.to_datetime(loginactivity_internal["ShiftCreationDateConv"])
    loginactivity_internal.loc[loginactivity_internal["user_id"]==name,"LoginInterval"] = (pd.to_datetime(loginactivity_internal.loc[loginactivity_internal["user_id"]==name,"creation_date"]) \
        - pd.to_datetime(loginactivity_internal.loc[loginactivity_internal["user_id"]==name,"ShiftCreationDateConv"])).dt.days
    loginactivity_internal.loc[loginactivity_internal["user_id"]==name,"LastLoginDelay"] = (loginactivity_reportdate - loginactivity_internal.loc[loginactivity_internal["user_id"]==name,"creation_date"].max()).days

loginactivity_internal["LoginInterval"] = loginactivity_internal["LoginInterval"].fillna(0)
loginactivity_internal_dailyloginavg = loginactivity_internal.groupby(["user_id","creation_date"])[["record_id","index"]].count().groupby("user_id")["record_id"].mean()
loginactivity_internal_dailyloginavg = loginactivity_internal_dailyloginavg.rename("DailyLoginCountAvg")

loginactivity_internal_summary = loginactivity_internal.groupby("user_id").agg({
    "LoginInterval": ['mean', 'max'], "LastLoginDelay":['max']})
loginactivity_internal_summary.columns = ["LoginIntervalMean","LoginIntervalMax","DaysSinceLastLogin"]
loginactivity_internal_summary = pd.merge(loginactivity_internal_summary,loginactivity_internal_dailyloginavg,how = "left",on="user_id")

phoneactivity_selectedcols_monthlysummary = phoneactivity.groupby("employee_number").agg({
    "mo_mou_outgoing": ['mean'], "sms_sent":['mean'], "data_used_mb": ['mean'],
    "total_recharge_amount": ['mean']
    })

phoneactivity_selectedcols_monthlysummary.columns = ["MinutesMeanMonthly",
                                                     "SMSMeanMonthly",
                                                     "DataMeanMonthly",
                                                     "RechargeMeanMonthly"]
phoneactivity_selectedcols_monthlysummary =phoneactivity_selectedcols_monthlysummary.reset_index()

emailactivity_collist = ["email_address","send_count","receive_count","read_count","meeting_interacted_count"]
teamsactivity_collist = ["email_address","team_chat_message_count","private_chat_message_count","call_count","meetings_attended_count"]
mailboxusage_collist = ["email_address","item_count"]
sharepointactivity_collist = ["email_address","viewed_edited_file_count","visited_page_count","SyncSharedFileCount"]
loginactivity_internal_summary_collist = ["DailyLoginCountAvg"]

clusteringtable_branch = pd.merge(masterusermapping[["Email","employee_number","BranchCode"]],
    emailactivity[emailactivity_collist],how="inner",right_on="email_address",left_on="Email")\
    .merge(teamsactivity[teamsactivity_collist],how="outer",right_on="email_address",left_on="Email")\
    .merge(mailboxusage[mailboxusage_collist],how="outer",right_on="email_address",left_on="Email")\
    .merge(sharepointactivity[sharepointactivity_collist],how="outer",right_on="email_address",left_on="Email")\
    .merge(loginactivity_internal_summary[loginactivity_internal_summary_collist],how="outer",right_on="user_id",left_on="Email")\
    .merge(phoneactivity_selectedcols_monthlysummary,how="outer",on="employee_number")


clusteringtable_branch.drop(list(clusteringtable_branch.filter(regex='email_address_').columns),axis=1,inplace=True)
clusteringtable_branch = clusteringtable_branch[pd.notnull(clusteringtable_branch["BranchCode"])]
clusteringtable_branch = clusteringtable_branch[clusteringtable_branch["BranchCode"].str.len()==4]


clusteringtable_branch_meanaggregation = clusteringtable_branch.groupby("BranchCode")\
    [list(clusteringtable_branch.drop(columns=["Email","employee_number","BranchCode"]).columns)].mean()
clusteringtable_branch_meanaggregation.dropna(how="all",inplace=True) #if there is no data at all for a particular branch, drop row
clusteringtable_branch_meanaggregation = clusteringtable_branch_meanaggregation.fillna(0)

clusteringtable_branch_meanaggregation_clustered, clusteringtable_scaled_pca = PCA_plus_kmeans(clusteringtable_branch_meanaggregation,3,4)
#clustering_scatterplot(clusteringtable_branch_meanaggregation_clustered, clusteringtable_scaled_pca )

#clustered dataframe
clusteringtable_branch_meanaggregation_clustered_formerging= clusteringtable_branch_meanaggregation_clustered["Cluster"].to_frame()
clusteringtable_branch_meanaggregation_clustered_formerging.columns = ["BranchTechUsageCluster"]

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(clusteringtable_branch_meanaggregation_clustered_formerging,how="left",left_index=True,right_index=True)


#AuditActivityCluster - - branchcode agg
branchmappingtable = masterbranchmapping[["description","branch_code"]]
branchmappingtable.columns=["BranchName","BranchCode"]

auditactivity["auditee_name"] = auditactivity["auditee_name"].str.strip()
auditactivity_collist = ["auditee_name","risk_level","submission_date","review_period_from","review_period_to","audit_group"]

auditactivity = pd.merge(auditactivity[auditactivity_collist],branchmappingtable[["BranchName","BranchCode"]],
                                 how="left",left_on = "auditee_name", right_on="BranchName")

auditactivity["audit_group"] = auditactivity["audit_group"].str.replace(" 2022","")
auditactivity["AuditMonth"]=auditactivity["submission_date"].dt.month
auditactivity["AuditYear"]=auditactivity["submission_date"].dt.year

audit_timeseries = pd.pivot_table(auditactivity, values='auditee_name', index=['BranchCode',"AuditYear", "AuditMonth"],
                    columns='risk_level', aggfunc='count',fill_value=0).sort_index(level=['BranchCode','AuditYear','AuditMonth']).reset_index()

riskcategories = ["High","Medium","Low","Zero Tolerance"]
audit_timeseries_groupingtest= audit_timeseries.groupby(["BranchCode"])[riskcategories].sum()
audit_timeseries_groupingtest2 = round(audit_timeseries.groupby(["BranchCode"])["AuditYear"].size() / (((auditactivity["submission_date"].max()-auditactivity["submission_date"].min()).days)/365),2)
audit_timeseries_groupingtest3 = audit_timeseries_groupingtest.merge(audit_timeseries_groupingtest2,left_index=True,right_index=True)
audit_timeseries_groupingtest3 = audit_timeseries_groupingtest3.rename(columns={"AuditYear": "Avg. audits per year"})

audit_timeseries_groupingtest3_clustered, clusteringtable_scaled_pca = PCA_plus_kmeans(audit_timeseries_groupingtest3,2)
#clustering_scatterplot(audit_timeseries_groupingtest3_clustered, clusteringtable_scaled_pca )

#clustered dataframe
audit_timeseries_groupingtest3_clustered_formerging= audit_timeseries_groupingtest3_clustered["Cluster"].to_frame()
audit_timeseries_groupingtest3_clustered_formerging.columns = ["AuditActivityCluster"]

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(audit_timeseries_groupingtest3_clustered_formerging,how="left",left_index=True,right_index=True)
ALL_BRANCHCODE_AGG["AuditActivityCluster"] = ALL_BRANCHCODE_AGG["AuditActivityCluster"].fillna(np.nan)


#conflict data - branchcode agg

#get branch township mapping
conflictactivity_summary = pd.pivot_table(conflictactivity,index = "admin3",columns="event_type",values="year",aggfunc="count")

branchtownshipmapping_conflictsummary = masterbranchmapping[["branch_code","township_name"]].merge(conflictactivity_summary,how="left",
                                                                                         left_on = "township_name", right_index=True).fillna(0)

branchtownshipmapping_conflictsummary = branchtownshipmapping_conflictsummary.drop(columns = ["township_name"]).set_index("branch_code")

branchtownshipmapping_conflictsummary_clustered, clusteringtable_scaled_pca = PCA_plus_kmeans(branchtownshipmapping_conflictsummary,2)
#clustering_scatterplot(branchtownshipmapping_conflictsummary_clustered, clusteringtable_scaled_pca )

#clustered dataframe
branchtownshipmapping_conflictsummary_formerging= branchtownshipmapping_conflictsummary_clustered["Cluster"].to_frame()
branchtownshipmapping_conflictsummary_formerging.columns = ["ConflictActivityCluster"]

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(branchtownshipmapping_conflictsummary_formerging,how="outer",left_index=True,right_index=True)


#Poverty mapping (2019 census - district level) - branchcode agg

branchtownshipmapping_1 = pd.DataFrame()
branchtownshipmapping_1["TownName"] = masterbranchmapping["township_name"] + " Town"
branchtownshipmapping_1["branch_code"] = masterbranchmapping["branch_code"]
branchtownshipmapping_1 = branchtownshipmapping_1.merge(towndistrictmapping[["town_name_eng","district_name_eng"]],how="left",
                                                    left_on = "TownName", right_on = "town_name_eng")

branchtownshipmapping_1.loc[branchtownshipmapping_1["branch_code"]=="THPU","district_name_eng"]="Yangon (East)"
branchtownshipmapping_1.loc[branchtownshipmapping_1["branch_code"]=="PDYA","district_name_eng"]="Taunggyi"
branchtownshipmapping_1.loc[branchtownshipmapping_1["branch_code"]=="PILG","district_name_eng"]="Taunggyi"

census_2019_data_withbranchcode = branchtownshipmapping_1.merge(census_2019_data,how="left",
                                                                left_on = "district_name_eng", right_on = "union_name").set_index("branch_code")
census_2019_data_withbranchcode = census_2019_data_withbranchcode.drop(columns=["TownName", "town_name_eng","district_name_eng","id","union_name",
                                                                                "year","created_by","created_at","updated_by","updated_at"])
census_2019_data_withbranchcode = census_2019_data_withbranchcode.astype(float)
census_2019_data_withbranchcode = census_2019_data_withbranchcode[~census_2019_data_withbranchcode.index.duplicated(keep='first')]

census_2019_data_withbranchcode_clustered, clusteringtable_scaled_pca = PCA_plus_kmeans(census_2019_data_withbranchcode,3,7)
clustering_scatterplot(census_2019_data_withbranchcode_clustered, clusteringtable_scaled_pca)

#clustered dataframe
census_2019_data_withbranchcode_clustered_formerging= census_2019_data_withbranchcode_clustered["Cluster"].to_frame()
census_2019_data_withbranchcode_clustered_formerging.columns = ["PovertyLevelCluster"]

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(census_2019_data_withbranchcode_clustered_formerging,how="outer",left_index=True,right_index=True)

#BM external hire or not - branchcode agg
BM_externalhire_1 = BM_externalhire.merge(branchmappingtable,how="left",left_on ="branch_name", right_on ='BranchName'  )
BM_externalhire_1 = BM_externalhire_1[["BranchCode","BM_externalhire"]].set_index("BranchCode")
BM_externalhire_1 = BM_externalhire_1[~BM_externalhire_1.index.duplicated(keep='first')]
BM_externalhire["employee_number"] = BM_externalhire["employee_number"].astype(float) 

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(BM_externalhire_1,how="left",left_index=True,right_index=True)


#BM tax dependents data
BM_tax = BM_externalhire.merge(branchmappingtable,how="left",left_on ="branch_name", right_on ='BranchName'  )
BM_tax = BM_tax.merge(taxdata,how="left",on="employee_number")
BM_tax = BM_tax[["BranchCode","number_of_parent","number_of_child","has_spouse"]].set_index("BranchCode")
BM_tax.columns = ["BM_number_of_parent","BM_number_of_child","BM_has_spouse"]
BM_tax = BM_tax[~BM_tax.index.duplicated(keep='first')]

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(BM_tax,how="left",left_index=True,right_index=True)

#all branch management disciplinary data
positionlist = ["Credit Advisor","Assistant Branch Manager","Branch Manager"]
disciplinarydata.loc[disciplinarydata["branch_name"]=="Thein Phyu - THPU","branch_name"] = "Theinphyu - THPU"
disciplinarydata_pivot = pd.pivot_table(disciplinarydata[disciplinarydata["position"].isin(positionlist)], values='name', index='branch_name',columns='warning_type', aggfunc='count',fill_value=0)
disciplinarydata_pivot.drop(columns=["second letter warning","termination letter"],inplace=True) #very low counts <3
disciplinarydata_pivot = disciplinarydata_pivot.reset_index()
disciplinarydata_pivot["BranchCode"] = disciplinarydata_pivot["branch_name"].str[-4:]
disciplinarydata_pivot = disciplinarydata_pivot.drop(columns="branch_name")
disciplinarydata_pivot.loc[disciplinarydata_pivot["BranchCode"].str.contains("- U"),"BranchCode"] = "NYGU"
disciplinarydata_pivot["BranchCode"] = disciplinarydata_pivot["BranchCode"].str.upper()
disciplinarydata_pivot = disciplinarydata_pivot.set_index("BranchCode")
disciplinarydata_pivot = disciplinarydata_pivot.add_prefix("BMABMCA_")


ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(disciplinarydata_pivot,how="left",left_index=True,right_index=True)

# #BM employee survey results - as of 16/9, no BM data avail
# BMsurveyresults = employeesurveydata.merge(BM_externalhire,how="left",on="employee_number").drop(columns="BM_externalhire")
# BMsurveyresults = BMsurveyresults.merge(branchmappingtable,how="left",left_on ="branch_name", right_on ='BranchName').drop(columns="employee_number")
# BMsurveyresults = BMsurveyresults[BMsurveyresults["BranchCode"].notnull()].drop(columns=["branch_name","BranchName"]).set_index("BranchCode")

# ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(BMsurveyresults,how="left",left_index=True,right_index=True)

#BM MFI exp - branchcode agg
BM_MFIexperience_1 = BM_MFIexperience[["employee_number","service_year"]].merge(BM_externalhire,how="left",on="employee_number").drop(columns="BM_externalhire")
BM_MFIexperience_1 = BM_MFIexperience_1.merge(branchmappingtable,how="left",left_on ="branch_name", right_on ='BranchName').drop(columns="employee_number")
BM_MFIexperience_1 = BM_MFIexperience_1[BM_MFIexperience_1["BranchCode"].notnull()].drop(columns=["branch_name","BranchName"])
BM_MFIexperience_1["year_experience1"] = BM_MFIexperience_1["service_year"].replace("-","").str.extract(r'(\d{1,2})\sY').astype(float).fillna(0)
BM_MFIexperience_1["year_experience2"] = BM_MFIexperience_1["service_year"].replace("-","").str.extract(r'(\d{1,2})\syear').astype(float).fillna(0)
BM_MFIexperience_1["months_experience1"] = BM_MFIexperience_1["service_year"].replace("-","").str.extract(r'(\d{1,2})\sM').astype(float).fillna(0)
BM_MFIexperience_1["months_experience2"] = BM_MFIexperience_1["service_year"].replace("-","").str.extract(r'(\d{1,2})\smonth').astype(float).fillna(0)
BM_MFIexperience_1["BM_approx_MFIexperience_years"] = BM_MFIexperience_1["year_experience1"] + BM_MFIexperience_1["year_experience2"] + BM_MFIexperience_1["months_experience1"]/12 + BM_MFIexperience_1["months_experience2"]/12

BM_MFIexperience_summary = BM_MFIexperience_1.groupby("BranchCode")["BM_approx_MFIexperience_years"].sum().to_frame()

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(BM_MFIexperience_summary,how="left",left_index=True,right_index=True)

#avg branch monthly disb
loan_agg_office_month = m_loan.groupby(['client_office_id','loan_disbursed_year_month']).agg({ 
    'loan_principal_amount': 'sum'}).reset_index()

loan_agg_office_month = loan_agg_office_month.rename(columns={"loan_principal_amount": "loan_principal_sum_office_month"})

loan_agg_office = loan_agg_office_month.groupby(['client_office_id']).agg({ 
    'loan_principal_sum_office_month': 'mean'}).reset_index()

loan_agg_office = loan_agg_office.rename(columns={"loan_principal_sum_office_month": "loan_principal_monthlyavg_office"})
loan_agg_office = loan_agg_office.merge(masterbranchmapping[["branch_code","office_id"]],how="left",left_on="client_office_id",right_on="office_id")
loan_agg_office = loan_agg_office.drop(columns=["client_office_id","office_id"]).set_index("branch_code")

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(loan_agg_office,how="left",left_index=True,right_index=True)

#loan transaction data by office & operational months

loan_transaction_agg_office_1 = loan_transaction_agg_office.merge(m_office[["office_id","opening_date"]],how="left",
                                                                left_on = "loan_transaction_office_id", right_on = "office_id" )

loan_transaction_agg_office_1["months_open"] = round((dt.datetime.today() - loan_transaction_agg_office_1["opening_date"]).dt.days/30,0)
loan_transaction_agg_office_1["monthlyavg_loan_transaction_count_office"] = round(loan_transaction_agg_office_1["loan_transaction_count_office"] / loan_transaction_agg_office_1["months_open"],0)
loan_transaction_agg_office_1 = loan_transaction_agg_office_1[loan_transaction_agg_office_1["loan_transaction_office_id"]!=0]
loan_transaction_agg_office_1["loan_transaction_reversed_percent"] = round((loan_transaction_agg_office_1["loan_transaction_reversed_count_office"] / loan_transaction_agg_office_1["loan_transaction_count_office"]) * 100,1)
loan_transaction_agg_office_1.drop(columns = ["loan_transaction_count_office","loan_transaction_reversed_count_office","office_id",
                                            "opening_date"],inplace=True)
loan_transaction_agg_office_1 = loan_transaction_agg_office_1.merge(masterbranchmapping[["branch_code","office_id"]],how="left",left_on="loan_transaction_office_id",right_on="office_id")
loan_transaction_agg_office_1 = loan_transaction_agg_office_1.drop(columns=["loan_transaction_office_id","office_id"]).set_index("branch_code")

loan_transaction_agg_office_1["avg_monthly_tx_per_activeloan"] = round(loan_transaction_agg_office_1["monthlyavg_loan_transaction_count_office"] / activeportfolio_by_branch["branch_loan_count"],1)
loan_transaction_agg_office_1.drop(columns = "monthlyavg_loan_transaction_count_office",inplace=True)

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(loan_transaction_agg_office_1,how="left",left_index=True,right_index=True)

#branch caseload
ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(activeportfolio_by_branch,how="left",left_index=True,right_index=True)

#account closure performance

mean_ontime_closure_delay_days = acc_gl_closure.groupby(["office_id","closing_date"])["id"].count().to_frame().reset_index().groupby("office_id")["id"].mean().to_frame()
mean_ontime_closure_delay_days.columns=["mean_ontime_closure_delay_days"]

latest_closure_delay_days = acc_gl_closure.groupby("office_id")["last_closing_date_delay_days"].min().to_frame()

n_dayreopenings = acc_gl_closure.groupby("office_id")["is_deleted"].sum().to_frame()
n_dayreopenings = n_dayreopenings.merge(m_office[["office_id","opening_date"]],how="left",left_index=True,right_on="office_id").set_index("office_id")
n_dayreopenings["months_open"] = round((dt.datetime.today() - n_dayreopenings["opening_date"]).dt.days/30,0)
n_dayreopenings["mean_monthly_reopening"] = round(n_dayreopenings["is_deleted"] / n_dayreopenings["months_open"],1)
n_dayreopenings = n_dayreopenings[["mean_monthly_reopening"]]

mean_reopen_delay = acc_gl_closure[(acc_gl_closure["is_deleted"]==1)].groupby(["office_id"])["reopen_delay_days"].mean().to_frame()

acc_closure_perf = mean_ontime_closure_delay_days.merge(latest_closure_delay_days,how="outer",left_index=True,right_index=True).merge(
    n_dayreopenings,how="outer",left_index=True,right_index=True).merge(mean_reopen_delay,how="outer",left_index=True,right_index=True).merge(
        masterbranchmapping[["branch_code","office_id"]],how="right",left_index=True,right_on="office_id").drop(columns=["office_id"]).set_index("branch_code")

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(acc_closure_perf,how="left",left_index=True,right_index=True)

#digital payment data - branch agg
digital_payment_data_branchsummary = round(digital_payment_data.groupby("branch_code").agg({"loan_id": "count", "amount": ["sum","mean"]}),0)
digital_payment_data_branchsummary.columns = ["digital_tx_count","digital_tx_totalamount","digital_tx_avgamount"]
digital_payment_data_branchsummary = digital_payment_data_branchsummary.sort_values(by="digital_tx_count",ascending=False)

digital_payment_data_branchsummary["digital_tx_totalamount_percentof_portfolio"] = round((digital_payment_data_branchsummary["digital_tx_totalamount"] / activeportfolio_by_branch["branch_portfolio_amount"]) * 100,1)
digital_payment_data_branchsummary_formerging = digital_payment_data_branchsummary.drop(columns = ["digital_tx_count","digital_tx_totalamount"])
digital_payment_data_branchsummary_formerging = digital_payment_data_branchsummary_formerging.add_prefix("branch_")

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(digital_payment_data_branchsummary_formerging,how="left",left_index=True,right_index=True)

#latedays & paidpc by branch
hwa_late_day_fields_notstaffloan = hwa_late_day_fields[~hwa_late_day_fields["latedays_client_id"].isin(staffloan_clientidlist)]

latedays_agg_office = hwa_late_day_fields_notstaffloan.groupby(['latedays_office_code']).agg({
    'late_days_total':'mean',
    'latedays_reschedule_count':'mean',
    'latedays_paid_pc_paid': 'sum',
    'latedays_paid_pc_due_interestprincipal': 'sum'})

latedays_agg_office = latedays_agg_office.rename(columns={"late_days_total":"latedays_latedays_avg_office","latedays_reschedule_count":"latedays_reschedule_avg_office","latedays_paid_pc_paid": "latedays_paid_pc_paid_office","latedays_paid_pc_due_interestprincipal": "latedays_paid_pc_due_interestprincipal_office"})
latedays_agg_office['latedays_paid_pc_office'] = latedays_agg_office['latedays_paid_pc_paid_office']/latedays_agg_office['latedays_paid_pc_due_interestprincipal_office']

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(latedays_agg_office[['latedays_paid_pc_office',"latedays_reschedule_avg_office","latedays_latedays_avg_office"]],how="left",left_index=True,right_index=True)

#staff loan repayment perf by branch
hwa_late_day_fields_staffloan = hwa_late_day_fields[hwa_late_day_fields["latedays_client_id"].isin(staffloan_clientidlist)]

latedays_agg_staffloan_office = hwa_late_day_fields_staffloan.groupby(['latedays_office_code']).agg({
    'late_days_total':'mean',
    'latedays_reschedule_count':'mean',
    'latedays_paid_pc_paid': 'sum',
    'latedays_paid_pc_due_interestprincipal': 'sum',
    "latedays_client_id":"nunique"})

latedays_agg_staffloan_office.columns = ["late_days_total_staffloan_office","latedays_reschedule_staffloan_office","latedays_paid_pc_paid","latedays_paid_pc_due_interestprincipal","n_staffloans"]
latedays_agg_staffloan_office['latedays_paid_pc_staffloan_office'] = latedays_agg_staffloan_office['latedays_paid_pc_paid']/latedays_agg_staffloan_office['latedays_paid_pc_due_interestprincipal']

FO_count_tempdf = df_FO2[["branch_name","total_fo_count"]]
FO_count_tempdf[["branch_name","branch_code"]] = FO_count_tempdf["branch_name"].str.split(" - ",expand=True)  
FO_count_tempdf = FO_count_tempdf.drop(columns = "branch_name").set_index("branch_code")

latedays_agg_staffloan_office['staffloans_to_FOcount_ratio'] = round(latedays_agg_staffloan_office["n_staffloans"] / FO_count_tempdf["total_fo_count"],1)

latedays_agg_staffloan_office = latedays_agg_staffloan_office[["late_days_total_staffloan_office","latedays_reschedule_staffloan_office","latedays_paid_pc_staffloan_office","staffloans_to_FOcount_ratio"]]

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(latedays_agg_staffloan_office,how="left",left_index=True,right_index=True)


#ddfr completion rate by branch
hwa_ddfr_agg_branch = hwa_ddfr[hwa_ddfr["ddfr_loan_id"].notnull()].merge(m_loan[["loan_id","branch_code"]],how="left",left_on="ddfr_loan_id",right_on="loan_id")
hwa_ddfr_agg_branch = hwa_ddfr_agg_branch.drop_duplicates(subset=['ddfr_loan_id'], keep='last').groupby(['branch_code'])["ddfr_id"].count().to_frame()
hwa_ddfr_agg_branch["DDFR_percent_complete"] = 100*(hwa_ddfr_agg_branch["ddfr_id"] / activeportfolio_by_branch["branch_loan_count"])
hwa_ddfr_agg_branch.loc[hwa_ddfr_agg_branch["DDFR_percent_complete"] > 100] = 100
hwa_ddfr_agg_branch = hwa_ddfr_agg_branch.add_prefix("branch_")
ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(hwa_ddfr_agg_branch["branch_DDFR_percent_complete"],how="left",left_index=True,right_index=True)


#BetterHR

empeid_list = df_BM["employee_id"].tolist()


str_empe_ids = "','".join([str(id) for id in empeid_list])

df_JA = get_staff_last_movement(str_empe_ids)

df_BM["current_branch_effective_date"] = df_BM.apply(get_ja_effective_date, JA_data=df_JA, axis = 1)
df_BM["bm_tenure_current_branch_days"] = df_BM.apply(get_tenure_current_branch, axis = 1)
df_BM2 = df_BM.drop(columns = ['branch_name','service_join_date','current_branch_effective_date','employee_id'])

df_FO2_1 = pd.merge(df_FO2, df_BM2,on=["location_id"], how="left")

empeid_list = df_ABM["employee_id"].tolist()

str_empe_ids = "','".join([str(id) for id in empeid_list])

df_JA = get_staff_last_movement(str_empe_ids)

df_ABM["current_branch_effective_date"] = df_ABM.apply(get_ja_effective_date, JA_data=df_JA, axis = 1)
df_ABM["abm_tenure_current_branch_days"] = df_ABM.apply(get_tenure_current_branch, axis = 1)
df_ABM2 = df_ABM.drop(columns = ['service_join_date','current_branch_effective_date','employee_id'])

df_FO2_2 = pd.merge(df_FO2_1, df_ABM2,on=["location_id"], how="left")

empeid_list = df_CA["employee_id"].tolist()


str_empe_ids = "','".join([str(id) for id in empeid_list])

df_JA = get_staff_last_movement(str_empe_ids)

df_CA["current_branch_effective_date"] = df_CA.apply(get_ja_effective_date, JA_data=df_JA, axis = 1)
df_CA["ca_tenure_current_branch_days"] = df_CA.apply(get_tenure_current_branch, axis = 1)
df_CA_ = df_CA.drop(columns = ['service_join_date','current_branch_effective_date','employee_id'])

df_CA2 = pd.DataFrame()
df_CA2["ca_total"] = df_CA_.groupby('location_id').size()
df_CA2["ca_avg_age"] = df_CA_.groupby('location_id')["empe_age"].agg('mean')
df_CA2["ca_avg_tenure_hana_days"] = df_CA_.groupby('location_id')["tenure_hana_days"].agg('mean')
df_CA2["ca_avg_tenure_current_branch_days"] = df_CA_.groupby('location_id')["ca_tenure_current_branch_days"].agg('mean')
df_CA2.reset_index()

df_FO2_3 = pd.merge(df_FO2_2, df_CA2,on=["location_id"], how="left")

df_FO2_4 = df_FO2_3.drop(columns = ['location_id'])

df_FO2_4[["branch_name","branch_code"]] = df_FO2_4["branch_name"].str.split(" - ",expand=True)  
df_FO2_4 = df_FO2_4.drop(columns = "branch_name").set_index("branch_code")

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.merge(df_FO2_4,how="left",left_index=True,right_index=True)

#replace BM ABM with BM or ABM
ALL_BRANCHCODE_AGG['bmabm_gender'] = ALL_BRANCHCODE_AGG.filter(like='bm_gender').ffill(axis=1).iloc[:,-1]
ALL_BRANCHCODE_AGG['bmabm_age'] = ALL_BRANCHCODE_AGG.filter(like='bm_age').ffill(axis=1).iloc[:,-1]
ALL_BRANCHCODE_AGG['bmabm_tenure_hana_days'] = ALL_BRANCHCODE_AGG.filter(like='bm_tenure_hana_days').ffill(axis=1).iloc[:,-1]
ALL_BRANCHCODE_AGG['bmabm_tenure_current_branch_days'] = ALL_BRANCHCODE_AGG.filter(like='bm_tenure_current_branch_days').ffill(axis=1).iloc[:,-1]

ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.drop(columns = ["bm_gender", "bm_age", "bm_tenure_hana_days", "bm_tenure_current_branch_days","abm_gender", "abm_age", "abm_tenure_hana_days", "abm_tenure_current_branch_days"])

#savings
ALL_BRANCHCODE_AGG["branch_savings_percent_loan_atdisb"] = round(100 * saving_loanagg.groupby("branch_code")["savingsbalance_atdisbursement"].sum() /  saving_loanagg.groupby("branch_code")["loanbalance_atdisbursement_withcurrentdisb"].sum(),1)
current_savings_percent_portfolio_temp = m_saving_current.groupby("client_id")["account_balance_derived"].sum().to_frame().join(
                                          m_loan.groupby("loan_client_id")["principal_outstanding_derived"].sum().to_frame())
current_savings_percent_portfolio_temp=current_savings_percent_portfolio_temp.merge(m_loan[["loan_client_id","branch_code"]],how="left",left_index=True,right_on="loan_client_id")
current_savings_percent_portfolio_temp = current_savings_percent_portfolio_temp.groupby("branch_code")[["account_balance_derived","principal_outstanding_derived"]].sum()

ALL_BRANCHCODE_AGG["current_savings_percent_portfolio"] = round(100*current_savings_percent_portfolio_temp["account_balance_derived"]/current_savings_percent_portfolio_temp["principal_outstanding_derived"],1)



print("shape:")
display(ALL_BRANCHCODE_AGG.shape)

print("n duplicated indexes: " + str(sum(ALL_BRANCHCODE_AGG.index.duplicated())))

#FOID (staff_id) LEVEL AGGREGATIONS
#start with m_staff

ALL_STAFFID_AGG = m_staff[["staff_id"]].set_index("staff_id")


#BetterHR
df_FO3 = df_FO.merge(masterusermapping_foidnotnull[["employee_number","FOID"]],how="left",on="employee_number")
df_FO3 = df_FO3[df_FO3["FOID"].notnull()]
df_FO3 = df_FO3.drop(columns = ['status','employee_number','date_of_birth', 'nrc', 'service_join_date', 'terminated_date', 'resignation_date', 'employee_id','current_location_effective_date',"martial_status" ])
df_FO3 = df_FO3.set_index("FOID")
df_FO3 = df_FO3.add_prefix("FO_")
df_FO3 = df_FO3[~df_FO3.index.duplicated(keep="last")]

ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(df_FO3,how="left",left_index=True,right_index=True)


#cleanup
ALL_STAFFID_AGG=ALL_STAFFID_AGG[ALL_STAFFID_AGG["FO_empe_age"].notnull()] #remove all non-FOs from list

#FO from same state/div as branch?
FO_statemapping = masterusermapping_foidnotnull[["FOID","BranchCode"]].merge(masterbranchmapping[["branch_code","StateCode"]],how="left",left_on="BranchCode",right_on="branch_code").set_index("FOID")
FO_statemapping = FO_statemapping.merge(ALL_STAFFID_AGG["FO_nrc_state"],how="left",left_index=True,right_index=True)
FO_statemapping["FOstate_same_as_branchstate"] = (FO_statemapping["StateCode"].values==FO_statemapping["FO_nrc_state"].values)
FO_statemapping = FO_statemapping[["FOstate_same_as_branchstate"]]

ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(FO_statemapping,how="left",left_index=True,right_index=True)



#FOAvgMonthlyPhoneActivityCluster
phoneactivity_selectedcols_monthlyFOsummary = phoneactivity.groupby("FOID").agg({
    "mo_mou_outgoing": ['mean'], "sms_sent":['mean'], "data_used_mb": ['mean'],
    "total_recharge_amount": ['mean']
    })

phoneactivity_selectedcols_monthlyFOsummary.columns = ["MinutesMeanMonthly",
                                                     "SMSMeanMonthly",
                                                     "DataMeanMonthly",
                                                     "RechargeMeanMonthly"]

phoneactivity_selectedcols_monthlyFOsummary = phoneactivity_selectedcols_monthlyFOsummary.iloc[:-1,:]

phoneactivity_selectedcols_monthlyFOsummary_clustered, clusteringtable_scaled_pca = PCA_plus_kmeans(phoneactivity_selectedcols_monthlyFOsummary,2)
#clustering_scatterplot(phoneactivity_selectedcols_monthlyFOsummary_clustered, clusteringtable_scaled_pca )

#clustered dataframe
phoneactivity_selectedcols_monthlyFOsummary_clustered_formerging= phoneactivity_selectedcols_monthlyFOsummary_clustered["Cluster"].to_frame()
phoneactivity_selectedcols_monthlyFOsummary_clustered_formerging.columns = ["FOAvgMonthlyPhoneActivityCluster"]

ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(phoneactivity_selectedcols_monthlyFOsummary_clustered_formerging,how="left",left_index=True,right_index=True)


#tax dependents data
taxdata_1 = taxdata.merge(masterusermapping_foidnotnull[["employee_number","FOID"]],how="left",on="employee_number")
taxdata_1.drop(columns=["employee_number"],inplace=True)
taxdata_1 = taxdata_1[taxdata_1["FOID"].notnull()].set_index("FOID")
taxdata_1 = taxdata_1.add_prefix("FO_")
taxdata_1 = taxdata_1[~taxdata_1.index.duplicated(keep="last")]
ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(taxdata_1[["FO_number_of_parent","FO_number_of_child","FO_has_spouse"]],how="left",left_index=True,right_index=True)


#disciplinary data
positionlist = ["Field Officer","Individual Field Officer"]
disciplinarydata_pivot = pd.pivot_table(disciplinarydata[disciplinarydata["position"].isin(positionlist)], values='name', index='employee_number',columns='warning_type', aggfunc='count',fill_value=0)
disciplinarydata_pivot.drop(columns=["second letter warning","termination letter"],inplace=True) #very low counts <3
disciplinarydata_pivot = disciplinarydata_pivot.merge(masterusermapping_foidnotnull[["employee_number","FOID"]],how="left",on="employee_number")
disciplinarydata_pivot = disciplinarydata_pivot[disciplinarydata_pivot["FOID"].notnull()].set_index("FOID").drop(columns="employee_number")
disciplinarydata_pivot = disciplinarydata_pivot.add_prefix("FO_")

ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(disciplinarydata_pivot,how="left",left_index=True,right_index=True)



#employee survey results - removed as not relevant for old FOs
# employeesurveydata_1 = employeesurveydata.merge(masterusermapping_foidnotnull[["employee_number","FOID"]],how="left",on="employee_number")
# employeesurveydata_1.drop(columns=["employee_number"],inplace=True)
# employeesurveydata_1 = employeesurveydata_1[employeesurveydata_1["FOID"].notnull()].set_index("FOID")
# employeesurveydata_1=employeesurveydata_1.add_prefix("FO_")
# ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(employeesurveydata_1,how="left",left_index=True,right_index=True)


#FO-wise portfolio & caseload

#ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(activeportfolio_by_FO,how="left",left_index=True,right_index=True)

#FO-wise first year performance
firstyear_loan_agg_staff = m_loan[m_loan['loan_staff_firstyear']==1].groupby(['loan_staff_id']).agg({
    'loan_id': 'count', 
    'loan_principal_amount': 'sum'})

firstyear_loan_agg_staff = firstyear_loan_agg_staff.rename(columns={"loan_id": "firstyear_loan_count_staff", "loan_principal_amount": "firstyear_loan_principal_sum_staff"})

ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(firstyear_loan_agg_staff,how="left",left_index=True,right_index=True)

# #reassignment history
# assignment_agg_staff = m_loan_officer_assignment_history.groupby(['assignment_staff_id']).agg({
#     'assignment_case_reassigned': 'sum',
#     'assignment_reassigned_case': 'sum'})

# assignment_agg_staff = assignment_agg_staff.rename(columns={"assignment_case_reassigned": "assignment_case_reassigned_staff", "assignment_reassigned_case": "assignment_reassigned_case_staff"})

# ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(assignment_agg_staff,how="left",left_index=True,right_index=True)


#digital payment data - FO agg
digital_payment_data_FOsummary = round(digital_payment_data.groupby("loan_staff_id").agg({"loan_id": "count", "amount": ["sum","mean"]}),0)
digital_payment_data_FOsummary.columns = ["digital_tx_count","digital_tx_totalamount","digital_tx_avgamount"]
digital_payment_data_FOsummary = digital_payment_data_FOsummary.sort_values(by="digital_tx_count",ascending=False)

digital_payment_data_FOsummary["digital_tx_totalamount_percentof_portfolio"] = round((digital_payment_data_FOsummary["digital_tx_totalamount"] / activeportfolio_by_FO["FO_portfolio_amount"]) * 100,1)
digital_payment_data_FOsummary = digital_payment_data_FOsummary[digital_payment_data_FOsummary["digital_tx_totalamount_percentof_portfolio"]<100]

digital_payment_dataFOsummary_formerging = digital_payment_data_FOsummary.drop(columns = ["digital_tx_count","digital_tx_totalamount"])
digital_payment_dataFOsummary_formerging = digital_payment_dataFOsummary_formerging.add_prefix("FO_")


ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(digital_payment_dataFOsummary_formerging,how="left",left_index=True,right_index=True)

#FO to branch avg caseload comparison

# avgcaseload_calc_1 = activeportfolio_by_FO.merge(masterusermapping_foidnotnull[["FOID","BranchCode"]].set_index("FOID"),how="left",left_index=True,right_index=True)
# avgcaseload_calc_2 = m_loan[m_loan["loan_status_id"]==300].groupby("branch_code").agg({"loan_id":"count","loan_staff_id":"nunique"})
# avgcaseload_calc_2["avg_caseload"] = round(avgcaseload_calc_2["loan_id"] / avgcaseload_calc_2["loan_staff_id"],0)
# avgcaseload_calc_1 = avgcaseload_calc_1.merge(avgcaseload_calc_2[["avg_caseload"]],how="left",left_on="BranchCode",right_index=True)
# avgcaseload_calc_1["FO_branch_caseload_ratio"] = avgcaseload_calc_1["FO_caseload"] / avgcaseload_calc_1["avg_caseload"]

# ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(avgcaseload_calc_1["FO_branch_caseload_ratio"],how="left",left_index=True,right_index=True)

#ddfr completion rate by FO - removed as not relevant for old FOs
# hwa_ddfr_agg_staff = hwa_ddfr.drop_duplicates(subset=['ddfr_loan_id'], keep='last').groupby(['ddfr_staff_id'])["ddfr_id"].count().to_frame()
# hwa_ddfr_agg_staff["DDFR_percent_complete"] = 100*(hwa_ddfr_agg_staff["ddfr_id"] / activeportfolio_by_FO["FO_caseload"])
# hwa_ddfr_agg_staff.loc[hwa_ddfr_agg_staff["DDFR_percent_complete"] > 100] = 100
# hwa_ddfr_agg_staff = hwa_ddfr_agg_staff.add_prefix("FO_")
# ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(hwa_ddfr_agg_staff["FO_DDFR_percent_complete"],how="left",left_index=True,right_index=True)


#client loan repayment performance
hwa_late_day_fields_notstaffloan = hwa_late_day_fields[~hwa_late_day_fields["latedays_client_id"].isin(staffloan_clientidlist)]
hwa_late_day_fields2 = hwa_late_day_fields_notstaffloan.merge(m_loan[["loan_id","loan_staff_id"]],how="left",left_on="latedays_loan_id",right_on="loan_id")

latedays_agg_FO = hwa_late_day_fields2.groupby(['loan_staff_id']).agg({
    'late_days_total':'mean',
    'latedays_reschedule_count':'mean',
    'latedays_paid_pc_paid': 'sum',
    'latedays_paid_pc_due_interestprincipal': 'sum'})

latedays_agg_FO = latedays_agg_FO.rename(columns={"late_days_total":"latedays_latedays_avg_FO","latedays_reschedule_count":"latedays_reschedule_avg_FO","latedays_paid_pc_paid": "latedays_paid_pc_paid_FO","latedays_paid_pc_due_interestprincipal": "latedays_paid_pc_due_interestprincipal_FO"})
latedays_agg_FO['latedays_paid_pc_FO'] = latedays_agg_FO['latedays_paid_pc_paid_FO']/latedays_agg_FO['latedays_paid_pc_due_interestprincipal_FO']

ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(latedays_agg_FO[['latedays_paid_pc_FO',"latedays_reschedule_avg_FO","latedays_latedays_avg_FO"]],how="left",left_index=True,right_index=True)


#staff loan repayment perf
hwa_late_day_fields_staffloan = hwa_late_day_fields[hwa_late_day_fields["latedays_client_id"].isin(staffloan_clientidlist)]

latedays_agg_staffloan = hwa_late_day_fields_staffloan.groupby(['latedays_client_id']).agg({
    'late_days_total':'sum',
    'latedays_reschedule_count':'sum',
    'latedays_paid_pc_paid': 'sum',
    'latedays_paid_pc_due_interestprincipal': 'sum',
    "latedays_loan_id":"nunique"})

latedays_agg_staffloan.columns = ["latedays_total_staffloan","reschedule_total_staffloan","latedays_paid_pc_paid_staffloan","latedays_paid_pc_due_interestprincipal_staffloan","n_times_staffloan"]
latedays_agg_staffloan = latedays_agg_staffloan.merge(masterusermapping_clientidnotnull[masterusermapping_clientidnotnull["FOID"].notnull()],how="left",left_index=True,right_on="client_id")
latedays_agg_staffloan['latedays_paid_pc_staffloan'] = latedays_agg_staffloan['latedays_paid_pc_paid_staffloan']/latedays_agg_staffloan['latedays_paid_pc_due_interestprincipal_staffloan']
latedays_agg_staffloan = latedays_agg_staffloan[['FOID',"n_times_staffloan",'latedays_total_staffloan','reschedule_total_staffloan','latedays_paid_pc_staffloan']].set_index("FOID")

ALL_STAFFID_AGG = ALL_STAFFID_AGG.merge(latedays_agg_staffloan,how="left",left_index=True,right_index=True)



# #savings - current balance not relevant for resigned FOs
ALL_STAFFID_AGG["FO_savings_percent_loan_atdisb"] = round(100 * saving_loanagg.groupby("loan_staff_id")["savingsbalance_atdisbursement"].sum() /  saving_loanagg.groupby("loan_staff_id")["loanbalance_atdisbursement_withcurrentdisb"].sum(),1)

# current_savings_percent_portfolio_temp = m_saving_current.groupby("client_id")["account_balance_derived"].sum().to_frame().join(
#                                           m_loan.groupby("loan_client_id")["principal_outstanding_derived"].sum().to_frame())
# current_savings_percent_portfolio_temp=current_savings_percent_portfolio_temp.merge(m_loan[["loan_client_id","loan_staff_id"]],how="left",left_index=True,right_on="loan_client_id")
# current_savings_percent_portfolio_temp = current_savings_percent_portfolio_temp.groupby("loan_staff_id")[["account_balance_derived","principal_outstanding_derived"]].sum()

# ALL_STAFFID_AGG["current_savings_percent_portfolio"] = round(100*current_savings_percent_portfolio_temp["account_balance_derived"]/current_savings_percent_portfolio_temp["principal_outstanding_derived"],1)
# ALL_STAFFID_AGG["current_savings_percent_portfolio"] = ALL_STAFFID_AGG["current_savings_percent_portfolio"].replace(np.inf,0)



print("shape:")
display(ALL_STAFFID_AGG.shape)
print("n duplicated indexes: " + str(sum(ALL_STAFFID_AGG.index.duplicated())))

#CLIENT_ID LEVEL AGGREGATIONS

#start with m_client features
ALL_CLIENTID_AGG = m_client[['client_id','client_tenure','client_urban','client_address_change','client_ph_duplicated']].set_index("client_id")

#loan cycle count & past avg loan size
past_loan_agg_client = m_loan.groupby(['loan_client_id']).agg({
    'loan_id': 'count', 
    'loan_principal_amount': 'mean'}).reset_index()

past_loan_agg_client = past_loan_agg_client.rename(columns={"loan_id": "past_loan_count_client", "loan_principal_amount": "past_loan_principal_avg_client"})
past_loan_agg_client = past_loan_agg_client.set_index("loan_client_id")
ALL_CLIENTID_AGG = ALL_CLIENTID_AGG.merge(past_loan_agg_client,how="left",left_index=True,right_index=True)

#has client had interventions before?

intervention_agg_client = intervention_dataset.groupby(['latedays_client_id']).agg({
    'is_intervention': 'sum'}).reset_index()

intervention_agg_client = intervention_agg_client.rename(columns={"is_intervention": "intervention_loan_count_client"}).set_index("latedays_client_id")

ALL_CLIENTID_AGG = ALL_CLIENTID_AGG.merge(intervention_agg_client,how="left",left_index=True,right_index=True)

#loan repayment performance
hwa_late_day_fields_notstaffloan = hwa_late_day_fields[~hwa_late_day_fields["latedays_client_id"].isin(staffloan_clientidlist)]

latedays_agg_client = hwa_late_day_fields_notstaffloan.groupby(['latedays_client_id']).agg({
    'late_days_total':'mean',
    'latedays_reschedule_count':'mean',
    'latedays_paid_pc_paid': 'sum',
    'latedays_paid_pc_due_interestprincipal': 'sum'}).reset_index()

latedays_agg_client = latedays_agg_client.rename(columns={"late_days_total":"latedays_latedays_avg_client","latedays_reschedule_count":"latedays_reschedule_avg_client","latedays_paid_pc_paid": "latedays_paid_pc_paid_client","latedays_paid_pc_due_interestprincipal": "latedays_paid_pc_due_interestprincipal_client"})
latedays_agg_client['latedays_paid_pc_client'] = latedays_agg_client['latedays_paid_pc_paid_client']/latedays_agg_client['latedays_paid_pc_due_interestprincipal_client']

ALL_CLIENTID_AGG = ALL_CLIENTID_AGG.merge(latedays_agg_client[['latedays_paid_pc_client',"latedays_reschedule_avg_client","latedays_latedays_avg_client"]],how="left",left_index=True,right_index=True)

#savings
ALL_CLIENTID_AGG["client_savings_percent_loan_atdisb"] = round(100 * saving_loanagg.groupby("loan_client_id")["savingsbalance_atdisbursement"].sum() /  saving_loanagg.groupby("loan_client_id")["loanbalance_atdisbursement_withcurrentdisb"].sum(),1)

current_savings_percent_portfolio_temp = m_saving_current.groupby("client_id")["account_balance_derived"].sum().to_frame().join(
                                          m_loan.groupby("loan_client_id")["principal_outstanding_derived"].sum().to_frame())

ALL_CLIENTID_AGG["current_savings_percent_portfolio"] = round(100*current_savings_percent_portfolio_temp["account_balance_derived"]/current_savings_percent_portfolio_temp["principal_outstanding_derived"],1)
ALL_CLIENTID_AGG["current_savings_percent_portfolio"] = ALL_CLIENTID_AGG["current_savings_percent_portfolio"].replace(np.inf,0)


#savings digital payment %
savings_digital_transaction = digital_payment_data[(digital_payment_data['product_type_short']=='Compulsory Savings')|(digital_payment_data['product_type_short']=='Voluntary Savings')]\
  [['id','loan_id','client_id','branch_code','transaction_date','amount','product_type','product_type_short']]

digital_transaction_agg_saving = savings_digital_transaction.groupby(['client_id'])["amount"].sum().to_frame()
digital_transaction_agg_saving.columns = ["total_digitaltx_savings"]
ALL_CLIENTID_AGG = ALL_CLIENTID_AGG.merge(digital_transaction_agg_saving,how="left",left_index=True,right_index=True)


print("shape:")
display(ALL_CLIENTID_AGG.shape)
print("n duplicated indexes: " + str(sum(ALL_CLIENTID_AGG.index.duplicated())))

#LOAN_ID LEVEL AGGREGATIONS

ALL_LOANID_AGG = pd.DataFrame()
ALL_LOANID_AGG.index = m_loan[((m_loan["loan_status_id"]==600) | (m_loan["loan_status_id"]==300) | (m_loan["loan_status_id"]==601))&
                    (m_loan["loan_disbursedon_date"]>=dt.datetime(2020,1,1))]["loan_id"]


#digital payments
digital_payment_data_loans = digital_payment_data[~digital_payment_data["product_type_short"].str.contains("Savings")]
digital_payment_data_loansummary = round(digital_payment_data_loans.groupby("loan_id").agg({"loan_id": "count", "amount": ["sum","mean"]}),0)
digital_payment_data_loansummary.columns = ["loan_digital_tx_count","loan_digital_tx_totalamount","loan_digital_tx_avgamount"]
digital_payment_data_loansummary = digital_payment_data_loansummary.sort_values(by="loan_digital_tx_count",ascending=False)

activeloan_index = m_loan[m_loan["loan_status_id"]==300].groupby("loan_id")["loan_principal_amount"]

digital_payment_data_loansummary["digital_tx_totalamount_percentof_portfolio"] = round((digital_payment_data_loansummary["loan_digital_tx_totalamount"] \
                                                                                        / m_loan[m_loan["loan_status_id"]==300].groupby("loan_id")["loan_principal_amount"].mean()) * 100,1)
digital_payment_data_loansummary = digital_payment_data_loansummary[digital_payment_data_loansummary["digital_tx_totalamount_percentof_portfolio"]<100]

ALL_LOANID_AGG = ALL_LOANID_AGG.merge(digital_payment_data_loansummary,how="left",left_index=True,right_index=True)


#late days
latedays_agg_loan = hwa_late_day_fields[["latedays_loan_id","late_days_total","latedays_reschedule_count","latedays_paid_pc_paid","latedays_paid_pc_due_interestprincipal"]].set_index("latedays_loan_id")
latedays_agg_loan = latedays_agg_loan.astype(float)
latedays_agg_loan["paid_pc_loan"] = latedays_agg_loan["latedays_paid_pc_paid"] / latedays_agg_loan["latedays_paid_pc_due_interestprincipal"]
latedays_agg_loan = latedays_agg_loan[["late_days_total","latedays_reschedule_count","paid_pc_loan"]]
ALL_LOANID_AGG = ALL_LOANID_AGG.merge(latedays_agg_loan,how="left",left_index=True,right_index=True)


#intervention
ALL_LOANID_AGG = ALL_LOANID_AGG.merge(intervention_dataset.set_index("latedays_loan_id")["is_intervention"],how="left",left_index=True,right_index=True)

#note: DDFR purposely excluded as 2020 disbursed fraud case loans will not have ddfr

#savings
ALL_LOANID_AGG["savings_loans_percent_atdisbursement"] = saving_loanagg.set_index("loan_id")["savings_loans_percent_atdisbursement"]


#fraud loan counts
ALL_LOANID_AGG.loc[ALL_LOANID_AGG.index.isin(fraudloanidlist),"is_fraudloan"] = 1
ALL_LOANID_AGG.loc[~ALL_LOANID_AGG.index.isin(fraudloanidlist),"is_fraudloan"] = 0
display(ALL_LOANID_AGG["is_fraudloan"].sum())
display(ALL_LOANID_AGG["is_fraudloan"].mean())

#reassignments, remove fraud loan related reassignments

ALL_LOANID_AGG["loan_reassignment_count_prefraud"] = (m_loan_officer_assignment_history.groupby("assignment_loan_id")["assignment_staff_id"].count()-1)
ALL_LOANID_AGG["loan_reassignment_count_prefraud"] = ALL_LOANID_AGG["loan_reassignment_count_prefraud"] - ALL_LOANID_AGG["is_fraudloan"]
ALL_LOANID_AGG.loc[ALL_LOANID_AGG["loan_reassignment_count_prefraud"]==-1,"loan_reassignment_count_prefraud"] = 0
ALL_LOANID_AGG.loc[ALL_LOANID_AGG["loan_reassignment_count_prefraud"]>50,"loan_reassignment_count_prefraud"] = 0
ALL_LOANID_AGG["loan_reassignment_count_prefraud"].value_counts()

#partial payments
loan_partial_pay = loan_repayment.merge(loan_transaction,how="left",left_on = "loan_id", right_on = "loan_id")
loan_partial_pay['transaction_count'] = loan_partial_pay['transaction_count'].fillna(0)
loan_partial_pay = loan_partial_pay.set_index("loan_id")
ALL_LOANID_AGG['n_partial_pay'] = (loan_partial_pay['transaction_count']-loan_partial_pay['due_repayment_count'])
ALL_LOANID_AGG['n_partial_pay'] = np.where(ALL_LOANID_AGG['n_partial_pay']<0,0,ALL_LOANID_AGG['n_partial_pay'])
ALL_LOANID_AGG.loc[ALL_LOANID_AGG['n_partial_pay']>100,"n_partial_pay"] = 0

# history of fraud
#get history of fraud at branch

# fraudloan_summary = fraudloanid_df[fraudloanid_df["Status"]!="Clear"].groupby(["branch_code","Fraud report date"]).agg({"loan_principal_amount":"sum", "finflux_loan_id":"count","CaseID":"nunique"})
# fraudloan_summary.columns=["fraudloan_principal_amount","fraud_loan_count","fraud_case_count"]
# fraudloan_summary["fraudloan_amount_percent_portfolio"] = round(fraudloan_summary["fraudloan_principal_amount"]/activeportfolio_by_branch["branch_portfolio_amount"]*100,1)
# fraudloan_summary["fraudloan_count_percent_portfolio"] = round(fraudloan_summary["fraud_loan_count"]/activeportfolio_by_branch["branch_loan_count"]*100,1)
# fraudloan_summary["cumulative_fraudloan_amount_percent_portfolio"] = fraudloan_summary.groupby("branch_code")["fraudloan_amount_percent_portfolio"].cumsum()
# fraudloan_summary["cumulative_fraudloan_count_percent_portfolio"] = fraudloan_summary.groupby("branch_code")["fraudloan_count_percent_portfolio"].cumsum()

#earliestfrauddate = min(fraudloan_summary.index.get_level_values(1))
# temp_df =  m_loan[((m_loan["loan_status_id"]==600) | (m_loan["loan_status_id"]==300) | (m_loan["loan_status_id"]==601))&(m_loan["loan_disbursedon_date"]>=dt.datetime(2020,1,1))]["loan_id,"loan_disbursedon_date"]
# scoring_df.loc[scoring_df["loan_disbursedon_date"]>earliestfrauddate,"branchagg_past_branchfraud_atdisbursementdate"] = \
#   scoring_df[(scoring_df["loan_disbursedon_date"]>earliestfrauddate)].parallel_apply(count_past_branchfraud,fraudloan_summary=fraudloan_summary, axis = 1)
#scoring_df["branchagg_past_branchfraud_atdisbursementdate"] = scoring_df["branchagg_past_branchfraud_atdisbursementdate"].fillna(0)

#scoring_df[["loan_id","branchagg_past_branchfraud_atdisbursementdate"]].to_csv("/content/drive/My Drive/Processing Inputs/branchagg_past_branchfraud_atdisbursementdate.csv")

loanagg_past_branchfraud_atdisbursementdate_df = pd.read_csv("/content/drive/My Drive/Processing Inputs/branchagg_past_branchfraud_atdisbursementdate.csv")
loanagg_past_branchfraud_atdisbursementdate_df = loanagg_past_branchfraud_atdisbursementdate_df.drop(columns="Unnamed: 0").set_index("loan_id")
loanagg_past_branchfraud_atdisbursementdate_df.columns = ["past_branchfraud_atdisbursementdate"]
ALL_LOANID_AGG = ALL_LOANID_AGG.merge(loanagg_past_branchfraud_atdisbursementdate_df,how="left",left_index=True,right_index=True)


print("shape:")
display(ALL_LOANID_AGG.shape)
print("n duplicated indexes: " + str(sum(ALL_LOANID_AGG.index.duplicated())))

#combine

scoring_df = m_loan[((m_loan["loan_status_id"]==600) | (m_loan["loan_status_id"]==300) | (m_loan["loan_status_id"]==601))&
                    (m_loan["loan_disbursedon_date"]>=dt.datetime(2020,1,1))][[
    "loan_id", "loan_client_id", "loan_staff_id", "branch_code", "loan_disbursedon_date"]]

display(scoring_df.shape)


ALL_BRANCHCODE_AGG = ALL_BRANCHCODE_AGG.add_prefix("branchagg_")
ALL_STAFFID_AGG = ALL_STAFFID_AGG.add_prefix("FOIDagg_")
ALL_CLIENTID_AGG = ALL_CLIENTID_AGG.add_prefix("clientidagg_")
ALL_LOANID_AGG = ALL_LOANID_AGG.add_prefix("loanidagg_")

scoring_df = scoring_df.merge(ALL_BRANCHCODE_AGG,how="left",left_on="branch_code",right_index=True,suffixes ="_branch")
display(scoring_df.shape)

scoring_df["loan_staff_id"] = scoring_df["loan_staff_id"].astype(float)
scoring_df = scoring_df.merge(ALL_STAFFID_AGG, how="left",left_on="loan_staff_id",right_index=True,suffixes ="FOID")
display(scoring_df.shape)

scoring_df = scoring_df.merge(ALL_CLIENTID_AGG, how="left",left_on="loan_client_id",right_index=True,suffixes ="clientid")
display(scoring_df.shape)

scoring_df = scoring_df.merge(ALL_LOANID_AGG, how="left",left_on="loan_id",right_index=True,suffixes ="_loanid")
display(scoring_df.shape)

scoring_df = scoring_df[scoring_df["loan_disbursedon_date"].notnull()]
display(scoring_df.shape)

scoring_df = scoring_df[scoring_df["loan_staff_id"]!=0]
display(scoring_df.shape)

#checking data completion

scoring_df = scoring_df.fillna(np.nan)
fillna_withzero_cols = ["branchagg_BMABMCA_final warning","branchagg_BMABMCA_first letter warning","branchagg_BMABMCA_verbal warning","branchagg_BM_approx_MFIexperience_years",
                       "FOIDagg_FO_final warning", "FOIDagg_FO_first letter warning", "FOIDagg_FO_verbal warning", "FOIDagg_FO_digital_tx_avgamount","FOIDagg_FO_digital_tx_totalamount_percentof_portfolio",
                       "FOIDagg_latedays_reschedule_avg_FO",
                       "FOIDagg_latedays_latedays_avg_FO", "loanidagg_loan_digital_tx_count","loanidagg_loan_digital_tx_totalamount","loanidagg_loan_digital_tx_avgamount",
                       "loanidagg_digital_tx_totalamount_percentof_portfolio","FOIDagg_latedays_total_staffloan","FOIDagg_reschedule_total_staffloan",
                        'branchagg_branch_savings_percent_loan_atdisb', 'branchagg_current_savings_percent_portfolio', 'FOIDagg_FO_savings_percent_loan_atdisb',
                        'clientidagg_client_savings_percent_loan_atdisb', 'clientidagg_current_savings_percent_portfolio','loanidagg_savings_loans_percent_atdisbursement']
fillna_withone_cols = ["loanidagg_paid_pc_loan","FOIDagg_latedays_paid_pc_FO","clientidagg_latedays_paid_pc_client","FOIDagg_latedays_paid_pc_staffloan"]

scoring_df[fillna_withzero_cols] = scoring_df[fillna_withzero_cols].fillna(0)
scoring_df[fillna_withone_cols] = scoring_df[fillna_withone_cols].fillna(1)
 
scoring_df["row_na_count"] = round(scoring_df.isna().sum(axis=1)/scoring_df.shape[1]*100,1)
plt.hist(scoring_df["row_na_count"])
plt.title("% of rows with nan - distribution")


#scoring_df.isna().sum().to_frame().to_csv('2.csv')
#scoring_df[scoring_df["row_na_count"]>15].to_csv("1.csv")
scoring_df = scoring_df.drop(columns="row_na_count")

#define lists
#check feature names

index_list = ["loan_id","loan_client_id","loan_staff_id","branch_code","loan_disbursedon_date"]
binary_categorical_list = ["FOIDagg_FOAvgMonthlyPhoneActivityCluster","branchagg_BranchTechUsageCluster","branchagg_AuditActivityCluster","branchagg_ConflictActivityCluster","branchagg_PovertyLevelCluster",
                           "branchagg_BM_externalhire","branchagg_bmabm_gender","FOIDagg_FO_gender","FOIDagg_FO_has_spouse","FOIDagg_FOstate_same_as_branchstate","clientidagg_client_urban",
                           "loanidagg_is_intervention","branchagg_BM_has_spouse","FOIDagg_FO_nrc_state"]
output_list = ["loanidagg_is_fraudloan"]
numerical_list = list(set(scoring_df.columns) - set(index_list) - set(binary_categorical_list) - set(output_list))
output = scoring_df[output_list]

scoring_df[numerical_list] = scoring_df[numerical_list].astype(float)
scoring_df[numerical_list].info(verbose=True)

scoring_df[binary_categorical_list].info(verbose=True)

scoring_df[output_list].info(verbose=True)

#CHECK
if (len(binary_categorical_list) + len(numerical_list)+len(output_list)) - (ALL_BRANCHCODE_AGG.shape[1] + ALL_STAFFID_AGG.shape[1] +ALL_CLIENTID_AGG.shape[1] + ALL_LOANID_AGG.shape[1]) == 0:
  print("column checksum OK")
else:
  print("column categorisation count error - please check")

"""# Scoring"""

#One hot encoding

dummy_df = pd.DataFrame() #we will use this to hold the final dummy data
current_dummy_df = pd.DataFrame() #we will use this to hold the dummy data currently being processed
dummylist = binary_categorical_list

#create and append dummy columns
for column_name in dummylist: #loop through only specific columns of the dataframe
  if scoring_df[column_name].isna().sum()>0:
    current_dummy_df = pd.get_dummies(scoring_df[column_name],drop_first=False, prefix = column_name, prefix_sep = "__", dummy_na=True) #use __ notation to work with scorecard generation code later in code
  else:
    current_dummy_df = pd.get_dummies(scoring_df[column_name],drop_first=False, prefix = column_name, prefix_sep = "__", dummy_na=False)
  dummy_df = pd.concat([dummy_df,current_dummy_df],axis=1)

#append numerical to dummy data
for column_name in numerical_list:
  dummy_df = pd.concat([dummy_df,scoring_df[column_name]],axis=1)


#remove non-compliant characters in column names to allow XGBoost to run
dummy_df= dummy_df.rename(columns=lambda s: s.replace("]", ")"))
dummy_df= dummy_df.rename(columns=lambda s: s.replace("<", "less_than"))


#separate train and test
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(dummy_df,output,test_size = 0.4,random_state=123)

# ## Set feature interaction constraints such that one tree only contains one feature / variable
# x_constraints = []
# interaction_constraints = []

# dat_cols = pd.Index(x_train.columns)
# combined_list = numerical_list+binary_categorical_list
# combined_list = [sub.replace("]", ")") for sub in combined_list]
# combined_list = [sub.replace("<", "less_than") for sub in combined_list]
# ind_vars = sorted(combined_list)

# for iv in ind_vars:
#     if np.any(dat_cols.isin([iv])):
#         f_var = [dat_cols.get_loc(iv)]
#         f_var_name = [dat_cols[f_var[0]]]
#     else:
#         f_var = []
#         f_var_name = []
    
#     x_constraints.append(f_var_name + list(dat_cols[np.where(dat_cols.str.startswith(iv+'__'))[0]]) )
#     interaction_constraints.append(f_var + list(np.where(dat_cols.str.startswith(iv+'__'))[0]) )


# #simplified XGBoost hypertuning GridSearch with shallow trees (max_depth = 3)
# params = {
#     'base_score': [output.mean()[0]],
#     'gamma': [1]               ## def: 0
#     , 'learning_rate': [0.05]     ## def: 0.1
#     , 'max_depth': [5]
#     , 'min_child_weight': [100]  ## def: 1
#     , 'n_estimators': [1000]
#     , 'objective': ['binary:logistic']
#     , 'reg_alpha': [0]
#     , 'reg_lambda': [1]          ## def: 1
#     , 'verbosity': [0]
#     , #'interaction_constraints': [interaction_constraints]
#     , 'eval_metric': ['auc']
#     , 'tree_method': ['hist']    ## def: 'auto'
# }


# print('now optimising model')
# from xgboost import XGBClassifier
# from sklearn.model_selection import GridSearchCV
# grid_search= GridSearchCV(XGBClassifier(scale_pos_weight=250),param_grid=params,cv=5,scoring='roc_auc',n_jobs=-1,verbose=0)
# grid_search.fit(x_train,y_train, eval_set=[(x_test, y_test)], early_stopping_rounds=100)

# print(grid_search.best_params_)
# print(grid_search.best_score_)

# bst_params = grid_search.best_params_


#full XGBoost with Bayesian search hypertuning


search_space = {
    "learning_rate": Real(0.01,0.1, "log-uniform"),
    "min_child_weight": Integer(10, 200, "uniform"),
    "max_depth": Integer(5,6, "uniform"),
    "subsample": Real(0.01, 1.0, "uniform"),
    "colsample_bytree": Real(0.01, 1.0, "log-uniform"),
    "colsample_bylevel": Real(0.01, 1.0, "log-uniform"),
    "reg_lambda": Real(0.1, 5, "log-uniform"),
    "reg_alpha": Real(0.1, 5, "log-uniform"),
    "gamma": Real(0.1, 2, "log-uniform"),
  "n_estimators" : Categorical([500]),
  "eval_metric" : Categorical(['auc']),
    "base_score" : Categorical([output.mean()[0]]),
    "tree_method" :Categorical(['gpu_hist']), #need GPU
    "objective" : Categorical(['binary:logistic']), 
    "verbosity" : Categorical([1])
}

print('now optimising model')

from xgboost import XGBClassifier

from skopt import BayesSearchCV 
bayes=BayesSearchCV(XGBClassifier(scale_pos_weight=250),search_space,n_iter=20,scoring='roc_auc',cv=3,random_state=42,n_jobs=-1,verbose=10)


from skopt.utils import point_asdict
def cb(v):
    last_point = v.x_iters[-1]
    p = point_asdict(search_space, last_point)
    print(p)


bayes.fit(x_train,y_train,eval_set=[(x_test, y_test)], early_stopping_rounds=10, callback=cb)

print(bayes.best_params_)
print(bayes.best_score_)
bst_params = bayes.best_params_


print('now creating final model with optimised parameters')
from xgboost import XGBClassifier
xgbmodel = XGBClassifier(scale_pos_weight=250,**bst_params, use_label_encoder=False)
#xgbmodel = XGBClassifier(**bst_params, use_label_encoder=False)
xgbmodel.fit(x_train, y_train, eval_set=[(x_test, y_test)], early_stopping_rounds=10)
bstr = xgbmodel.get_booster()

print((bstr.best_score, bstr.best_iteration, bstr.best_ntree_limit))

# import pickle
# pickle.dump(xgbmodel, open("/content/drive/My Drive/Processing Inputs/MLDF.model", "wb"))


xgbmodel.save_model("/content/drive/My Drive/Processing Inputs/MLDF.json")

# ## Create Scorecard
# mdf = bstr.trees_to_dataframe()
# mdf = mdf.loc[mdf['Tree'] <= bstr.best_iteration]
# mdf_parents = mdf[mdf.Feature!='Leaf'].drop(columns=['Tree','Gain','Cover'])
# mdf_leafs = mdf[mdf.Feature=='Leaf'].drop(columns=['Feature','Split','Yes','No','Missing','Cover'])
# mdf_leafs.rename(columns={'ID': 'ID0', 'Node': 'Node0'}, inplace=True)

# tree_traceback = pd.DataFrame()
# itr = 0 
# itrs = str(itr)
# while mdf_leafs.shape[0] > 0: #while rows of mdf_leafs > 0
#     NoSprout = pd.merge(mdf_leafs, mdf_parents, how='inner', left_on='ID'+itrs, right_on='No')
#     YesSprout = pd.merge(mdf_leafs, mdf_parents, how='inner', left_on='ID'+itrs, right_on='Yes')
#     MissingSprout = pd.merge(mdf_leafs, mdf_parents, how='inner', left_on='ID'+itrs, right_on='Missing')
#     MissingSprout.Split = np.nan
    
#     itr += 1
#     itrs = str(itr)    
#     NoSprout.insert(NoSprout.shape[1]-4, 'Sign'+itrs, '>=')
#     YesSprout.insert(YesSprout.shape[1]-4, 'Sign'+itrs, '<')
#     MissingSprout.insert(MissingSprout.shape[1]-4, 'Sign'+itrs, '.')
#     mdf_leafs = pd.concat([NoSprout, YesSprout, MissingSprout]) #row-wise concat
#     mdf_leafs.rename(columns={'ID':'ID'+itrs, 'Split':'Split'+itrs, 'Feature':'Feature'+itrs, 'Node':'Node'+itrs, 
#                             'Yes':'Yes'+itrs, 'No':'No'+itrs, 'Missing':'Missing'+itrs}, inplace=True)
    
#     tree_traceback = tree_traceback.append(mdf_leafs.loc[mdf_leafs['Node'+itrs]==0,:], sort=False)
#     mdf_leafs = mdf_leafs[mdf_leafs['Node'+itrs]!=0]

# ttb_missing = tree_traceback.copy()
# ttb_non_missing = tree_traceback.copy()
# for i in range(1,itr+1): 
#     ttb_missing = ttb_missing[(ttb_missing['Sign'+str(i)] == '.') | ttb_missing['Sign'+str(i)].isna()]
#     ttb_non_missing = ttb_non_missing[ttb_non_missing['Sign'+str(i)] != '.']

# ttb = ttb_non_missing.copy()
# ttb.sort_values(['Tree', 'Split1', 'Sign1'], inplace=True, na_position='first')
# ttb.reset_index(drop=True, inplace=True)

# sc_df = ttb.iloc[:,:4].rename(columns={'ID0':'ID', 'Node0':'Node', 'Gain':'XAddEvidence'}).copy()
# sc_df['Feature'] = ttb.Feature1.values
# sc_df['Sign'] = ttb.Sign1.values
# sc_df['Split'] = ttb.Split1.values

# for i in range(1,itr): 
#     replace_in_sc = ( ( sc_df['Sign']=='>=').values 
#                         & (ttb['Split'+str(i)] < ttb['Split'+str(i+1)]).values 
#                         & (ttb['Feature'+str(i)] == ttb['Feature'+str(i+1)]).values ) #generate boolean array meeting this condition
#     sc_df.loc[replace_in_sc,'Sign'] = ttb['Sign'+str(i+1)][replace_in_sc].values
#     sc_df.loc[replace_in_sc,'Split'] = ttb['Split'+str(i+1)][replace_in_sc].values

# sc_df['Inc_Missing'] = sc_df.ID.isin(ttb_missing.ID0).astype(int)    

# ## Move 'XAddEvidence' to the far right of sc_df
# cols = sc_df.columns.to_list()
# cols.pop(cols.index('XAddEvidence')) 
# sc_df = sc_df[cols+['XAddEvidence']] 

# ## Reformat categorical variables
# OTHER_CAT_IND = "OTHER"  ## The label for the all other items in categorical variable
# feature_decomp = sc_df.Feature.str.split('__', n=1, expand=True)
# if feature_decomp.shape[1] > 1: #run below only if there are categorical features in top x iterations....
#   cat_rows = ~feature_decomp[1].isna() #tilde is invert - to identify only categorical rows
#   other_cat_rows = (cat_rows & (sc_df['Sign'] == '<')).values #< operator on a feature category means "OTHER"
#   feat_categories = feature_decomp.iloc[:,1].copy()
#   feat_categories.loc[other_cat_rows] = OTHER_CAT_IND
#   sc_df.loc[cat_rows, 'Split'] = feat_categories[cat_rows].values #amend only specific (categorical) rows and cols
#   sc_df.loc[cat_rows, 'Feature'] = feature_decomp[0][cat_rows].values
#   sc_df.loc[cat_rows, 'Sign'] = "="
#   sc_df.loc[cat_rows, 'Inc_Missing'] = 0

# sc_df.sort_values(['Tree', 'Sign', 'Split'], inplace=True, na_position='last')
# sc_df.set_index(['Tree'], inplace=True)

# scorecard = sc_df.drop(columns=['Node', 'ID'])

# display(scorecard)
# scorecard.to_csv("/content/drive/My Drive/Processing Inputs/sc.csv")

"""# SHAP Model Exploration"""

# #reload
# import pandas as pd

# scoring_df = pd.read_csv("/content/drive/My Drive/Processing Inputs/scoring_df.csv")
# scoring_df = scoring_df.drop(columns="Unnamed: 0")

# index_list = ["loan_id","loan_client_id","loan_staff_id","branch_code","loan_disbursedon_date"]
# binary_categorical_list = ["FOIDagg_FOAvgMonthlyPhoneActivityCluster","branchagg_BranchTechUsageCluster","branchagg_AuditActivityCluster","branchagg_ConflictActivityCluster","branchagg_PovertyLevelCluster",
#                            "branchagg_BM_externalhire","branchagg_bmabm_gender","FOIDagg_FO_gender","FOIDagg_FO_has_spouse","FOIDagg_FOstate_same_as_branchstate","clientidagg_client_urban",
#                            "loanidagg_is_intervention","branchagg_BM_has_spouse","FOIDagg_FO_nrc_state"]
# output_list = ["is_fraudloan"]
# numerical_list = list(set(scoring_df.columns) - set(index_list) - set(binary_categorical_list) - set(output_list))
# output = scoring_df[output_list]

# scoring_df[numerical_list] = scoring_df[numerical_list].astype(float)

# dummy_df = pd.DataFrame() #we will use this to hold the final dummy data
# current_dummy_df = pd.DataFrame() #we will use this to hold the dummy data currently being processed
# dummylist = binary_categorical_list

# #create and append dummy columns
# for column_name in dummylist: #loop through only specific columns of the dataframe
#   if scoring_df[column_name].isna().sum()>0:
#     current_dummy_df = pd.get_dummies(scoring_df[column_name],drop_first=False, prefix = column_name, prefix_sep = "__", dummy_na=True) #use __ notation to work with scorecard generation code later in code
#   else:
#     current_dummy_df = pd.get_dummies(scoring_df[column_name],drop_first=False, prefix = column_name, prefix_sep = "__", dummy_na=False)
#   dummy_df = pd.concat([dummy_df,current_dummy_df],axis=1)

# #append numerical to dummy data
# for column_name in numerical_list:
#   dummy_df = pd.concat([dummy_df,scoring_df[column_name]],axis=1)


# #remove non-compliant characters in column names to allow XGBoost to run
# dummy_df= dummy_df.rename(columns=lambda s: s.replace("]", ")"))
# dummy_df= dummy_df.rename(columns=lambda s: s.replace("<", "less_than"))


# #separate train and test
# from sklearn.model_selection import train_test_split
# x_train, x_test, y_train, y_test = train_test_split(dummy_df,output,test_size = 0.4,random_state=123)

# # import pickle
# # xgbmodel = pickle.load(open("/content/drive/My Drive/Processing Inputs/MLDF.model","rb"))

# from xgboost import XGBClassifier
# xgbmodel_loaded = XGBClassifier()
# xgbmodel_loaded.load_model("/content/drive/My Drive/Processing Inputs/MLDF.json")
# xgbmodel_loaded.fit(x_train, y_train, eval_set=[(x_test, y_test)], early_stopping_rounds=10)

#SHAP explanation
#note: needs latest xgboost version - does not work with colab default version 0.9.0
!pip install shap
import shap

explainer = shap.TreeExplainer(xgbmodel) 
x_sampled = x_test
#x_sampled = x_test.sample(100000, random_state=10)
shap_values = explainer.shap_values(x_sampled)


#shap summary plot
shap.summary_plot(shap_values, x_sampled,max_display=15)

# #shap summary plot bar
# shap.summary_plot(shap_values, x_sampled, plot_type="bar")

# scoring_df = scoring_df.drop(columns="Pf1")
# scoring_df.to_csv("/content/drive/My Drive/Processing Inputs/scoring_df.csv")

"""# Prediction & prioritisation"""

dummy_df = dummy_df[xgbmodel.get_booster().feature_names]
scoring_df["Pf1"] = xgbmodel.predict_proba(dummy_df)[:,1]

display(scoring_df["Pf1"].mean())


branch_priority = scoring_df.groupby("branch_code").agg({"Pf1":"mean","loanidagg_is_fraudloan":"sum"}).sort_values(by="Pf1",ascending=False)
branch_priority.columns=["Mean Pf1","past_fraud_detected_at_br"]
branch_priority["past_fraud_detected_at_br"] = branch_priority["past_fraud_detected_at_br"]>0
branch_priority_1 = branch_priority.groupby("past_fraud_detected_at_br")["Mean Pf1"].mean().to_frame()
# branch_priority_1.plot(kind="bar",title='Fraud detection model prediction capability', ylabel='predicted: branch avg. Pf (ex-ante)',
#          xlabel='actual: has fraud happened at branch before?', figsize=(6, 5))
display(print("branch Pf separation = " + str(round(branch_priority_1.loc[True][0]/branch_priority_1.loc[False][0],1))))

FO_priority = scoring_df.groupby("loan_staff_id").agg({"Pf1":"mean","loanidagg_is_fraudloan":"sum"}).sort_values(by="Pf1",ascending=False)
FO_priority.columns=["Mean Pf1","past_fraud_detected_at_FO"]
FO_priority["past_fraud_detected_at_FO"] = FO_priority["past_fraud_detected_at_FO"]>0
FO_priority_1 = FO_priority.groupby("past_fraud_detected_at_FO")["Mean Pf1"].mean().to_frame()
# FO_priority_1.plot(kind="bar",title='Fraud detection model prediction capability', ylabel='predicted: FO avg. Pf (ex-ante)',
#          xlabel='actual: has fraud happened at FO before?', figsize=(6, 5))
display(print("FO Pf separation = " + str(round(FO_priority_1.loc[True][0]/FO_priority_1.loc[False][0],1))))

client_priority = scoring_df.groupby("loan_client_id").agg({"Pf1":"mean","loanidagg_is_fraudloan":"sum"}).sort_values(by="Pf1",ascending=False)
client_priority.columns=["Mean Pf1","past_fraud_detected_at_client"]
client_priority["past_fraud_detected_at_client"] = client_priority["past_fraud_detected_at_client"]>0
client_priority_1 = client_priority.groupby("past_fraud_detected_at_client")["Mean Pf1"].mean().to_frame()
# client_priority_1.plot(kind="bar",title='Fraud detection model prediction capability', ylabel='predicted: client avg. Pf (ex-ante)',
#          xlabel='actual: has fraud happened at client before?', figsize=(6, 5))
display(print("client separation = " + str(round(client_priority_1.loc[True][0]/client_priority_1.loc[False][0],1))))

loan_priority = scoring_df.groupby("loan_id").agg({"Pf1":"mean","loanidagg_is_fraudloan":"sum"}).sort_values(by="Pf1",ascending=False)
loan_priority.columns=["Mean Pf1","past_fraud_detected_at_loan"]
loan_priority["past_fraud_detected_at_loan"] = loan_priority["past_fraud_detected_at_loan"]>0
loan_priority_1 = loan_priority.groupby("past_fraud_detected_at_loan")["Mean Pf1"].mean().to_frame()
# loan_priority_1.plot(kind="bar",title='Fraud detection model prediction capability', ylabel='predicted: loan avg. Pf (ex-ante)',
#          xlabel='actual: has fraud happened at loan before?', figsize=(6, 5))
display(print("loan separation = " + str(round(loan_priority_1.loc[True][0]/loan_priority_1.loc[False][0],1))))

combined_plot_df = pd.concat([branch_priority_1,FO_priority_1,client_priority_1,loan_priority_1],axis=1)
combined_plot_df.columns=["branch_fraud_prediction","FO_fraud_prediction","client_fraud_prediction","loan_fraud_prediction"]
#combined_plot_df.plot(x=index, y=list(combined_plot_df.columns), kind="bar")
ax = combined_plot_df.plot.bar(subplots=True,figsize = (10,10),layout=(2,2),title = "Predictive capability of fraud detection model",
                      xlabel = "actual: has fraud happened before?", ylabel = "predicted: avg. Pf (ex-ante)")

#predictions on x_test

dummy_df_xtest = dummy_df.loc[x_test.index]
dummy_df_xtest = dummy_df_xtest[xgbmodel.get_booster().feature_names]
scoring_df_xtest = scoring_df.loc[x_test.index]
scoring_df_xtest["Pf1"] = xgbmodel.predict_proba(dummy_df_xtest)[:,1]


display(scoring_df_xtest["Pf1"].mean())


branch_priority = scoring_df_xtest.groupby("branch_code").agg({"Pf1":"mean","loanidagg_is_fraudloan":"sum"}).sort_values(by="Pf1",ascending=False)
branch_priority.columns=["Mean Pf1","past_fraud_detected_at_br"]
branch_priority["past_fraud_detected_at_br"] = branch_priority["past_fraud_detected_at_br"]>0
branch_priority_1 = branch_priority.groupby("past_fraud_detected_at_br")["Mean Pf1"].mean().to_frame()
# branch_priority_1.plot(kind="bar",title='Fraud detection model prediction capability', ylabel='predicted: branch avg. Pf (ex-ante)',
#          xlabel='actual: has fraud happened at branch before?', figsize=(6, 5))
display(print("branch Pf separation = " + str(round(branch_priority_1.loc[True][0]/branch_priority_1.loc[False][0],1))))

FO_priority = scoring_df_xtest.groupby("loan_staff_id").agg({"Pf1":"mean","loanidagg_is_fraudloan":"sum"}).sort_values(by="Pf1",ascending=False)
FO_priority.columns=["Mean Pf1","past_fraud_detected_at_FO"]
FO_priority["past_fraud_detected_at_FO"] = FO_priority["past_fraud_detected_at_FO"]>0
FO_priority_1 = FO_priority.groupby("past_fraud_detected_at_FO")["Mean Pf1"].mean().to_frame()
# FO_priority_1.plot(kind="bar",title='Fraud detection model prediction capability', ylabel='predicted: FO avg. Pf (ex-ante)',
#          xlabel='actual: has fraud happened at FO before?', figsize=(6, 5))
display(print("FO Pf separation = " + str(round(FO_priority_1.loc[True][0]/FO_priority_1.loc[False][0],1))))

client_priority = scoring_df_xtest.groupby("loan_client_id").agg({"Pf1":"mean","loanidagg_is_fraudloan":"sum"}).sort_values(by="Pf1",ascending=False)
client_priority.columns=["Mean Pf1","past_fraud_detected_at_client"]
client_priority["past_fraud_detected_at_client"] = client_priority["past_fraud_detected_at_client"]>0
client_priority_1 = client_priority.groupby("past_fraud_detected_at_client")["Mean Pf1"].mean().to_frame()
# client_priority_1.plot(kind="bar",title='Fraud detection model prediction capability', ylabel='predicted: client avg. Pf (ex-ante)',
#          xlabel='actual: has fraud happened at client before?', figsize=(6, 5))
display(print("client separation = " + str(round(client_priority_1.loc[True][0]/client_priority_1.loc[False][0],1))))

loan_priority = scoring_df_xtest.groupby("loan_id").agg({"Pf1":"mean","loanidagg_is_fraudloan":"sum"}).sort_values(by="Pf1",ascending=False)
loan_priority.columns=["Mean Pf1","past_fraud_detected_at_loan"]
loan_priority["past_fraud_detected_at_loan"] = loan_priority["past_fraud_detected_at_loan"]>0
loan_priority_1 = loan_priority.groupby("past_fraud_detected_at_loan")["Mean Pf1"].mean().to_frame()
# loan_priority_1.plot(kind="bar",title='Fraud detection model prediction capability', ylabel='predicted: loan avg. Pf (ex-ante)',
#          xlabel='actual: has fraud happened at loan before?', figsize=(6, 5))
display(print("loan separation = " + str(round(loan_priority_1.loc[True][0]/loan_priority_1.loc[False][0],1))))

combined_plot_df = pd.concat([branch_priority_1,FO_priority_1,client_priority_1,loan_priority_1],axis=1)
combined_plot_df.columns=["branch_fraud_prediction","FO_fraud_prediction","client_fraud_prediction","loan_fraud_prediction"]
#combined_plot_df.plot(x=index, y=list(combined_plot_df.columns), kind="bar")
ax = combined_plot_df.plot.bar(subplots=True,figsize = (10,10),layout=(2,2),title = "Predictive capability of fraud detection model",
                      xlabel = "actual: has fraud happened before?", ylabel = "predicted: avg. Pf (ex-ante)")

#Prioritisation
today=pd.to_datetime("today").strftime('%Y-%m-%d')

branch_priority.head(30)
branch_priority.to_csv("/content/drive/My Drive/Processing Inputs/MLFS_branchpriority_" + today + ".csv")

FO_priority_2 = FO_priority.merge(masterusermapping_foidnotnull[["FOID","BranchCode","status"]],how="left",left_index=True,right_on="FOID")
FO_priority_2 = FO_priority_2[FO_priority_2["status"]=="Active"].set_index("FOID")
FO_priority_2.head(30)
FO_priority_2.to_csv("/content/drive/My Drive/Processing Inputs/MLFS_FOpriority_" + today + ".csv")

loan_priority_2 = loan_priority.merge(m_loan[["loan_id","branch_code","loan_status_id"]],how="left",left_index=True,right_on="loan_id")
loan_priority_2 = loan_priority_2[loan_priority_2["loan_status_id"]==300].set_index("loan_id")
loan_priority_2.head(30)
loan_priority_2.to_csv("/content/drive/My Drive/Processing Inputs/MLFS_loanpriority_" + today + ".csv")

"""# Investigations"""

scoring_df.filter(like="child")

featureinvestigation("FOIDagg_FO_number_of_child")

plt.bar(x=["0","1","2","3","nan"],height=scoring_df.groupby("FOIDagg_FO_number_of_child",dropna=False)["loanidagg_is_fraudloan"].mean())
plt.bar(x=["0","1","2","3"],height=scoring_df.groupby("FOIDagg_FO_number_of_child",dropna=True)["loanidagg_is_fraudloan"].mean())
plt.ylabel("fraud rate")
plt.xlabel("n children")
#ALL_STAFFID_AGG["FOIDagg_FO_number_of_child"].value_counts()

ALL_STAFFID_AGG